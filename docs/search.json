[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "stochacalculus",
    "section": "",
    "text": "1 Ecuaciones Diferenciales Estocásticas\nAhora, vamos"
  },
  {
    "objectID": "Tarea1.html",
    "href": "Tarea1.html",
    "title": "2  Tarea 1",
    "section": "",
    "text": "Ejecute y explica en pocas palabras la salida del código ex_001.py\n\n\nfrom scipy.stats import multivariate_normal\nfrom mpl_toolkits.mplot3d import axes3d\nfrom scipy.stats import norm\nimport numpy as np\nfrom scipy.stats import bernoulli\nimport matplotlib.pyplot as plt\nfig_01, ax_01 = plt.subplots(1, 1)\nfig_02, ax_02 = plt.subplots(1, 1)\np = 0.3\nmean, var, skew, kurt = bernoulli.stats(p, moments='mvsk')\nprint(mean, var, skew,kurt)\n\nx = np.arange(bernoulli.ppf(0.01, p), bernoulli.ppf(0.99, p))\nax_01.plot(x, bernoulli.pmf(x, p), 'bo', ms=8, label='bernoulli pmf')\nax_01.vlines(x, 0, bernoulli.pmf(x, p), colors='b', lw=5, alpha=0.5)\nr = bernoulli.rvs(p, size=1000)\nax_02.hist(r, bins=200)\nplt.show()\n\n0.3 0.21 0.8728715609439694 -1.2380952380952381\n\n\n\n\n\n\n\n\nEl código posee 3 salidas: * Un vector \\([0.3, 0.21, 0.87, -1.23]\\) * Dos gráficas.\nEl vector hace referencia a los momentos de la distribución bernoulli con parámetro \\(p=0.3\\). * mean hace referencia a la media. * var hace referencia a la varianza. * skew hace referencia al sesgo. * kurt hace referencia a la kurtosis.\nFinalmente, las dos gráficas: * La primera hace referencia a la función de probabilidad. Notemos que \\(\\mathcal{P}[X = 0] = 0.7\\), lo que muestra la gráfica. Notemos que la gráfica va de -0.04 a 0.04, por lo tanto no se iba a mostrar el caso \\(X=1\\).\n* La segunda hace referencia a una simulación: Se generaron una muestra de tamaño \\(N\\) variables aleatorias con distribución bernoulli. Como la distribución bernoulli tiene media \\(Np\\), pretende mostrar que en efecto, habrá de forma aproximada \\(Np\\) valores igual a 1 y \\(N(1-p)\\) valores igual a 0.\n\nEjecute y explica en pocas palabras la salida del código ex_002.py\n\n\nfig, ax = plt.subplots(1, 1)\nmean, var, skew, kurt = norm.stats(moments='mvsk')\n\nx = np.linspace(norm.ppf(0.01), norm.ppf(0.99), 100)\nax.plot(\n    x,\n    norm.pdf(x),\n    'r-',\n    lw=5,\n    alpha=0.6,\n    label='norm pdf'\n)\nrv = norm()\nax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\nvals = norm.ppf([0.001, 0.5, 0.999])\n\nnp.allclose([0.001, 0.5, 0.999], norm.cdf(vals))\n\nr = norm.rvs(size=50000)\n\nax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\nax.set_xlim([x[0], x[-1]])\nax.legend(loc='best', frameon=False)\nplt.show()\n\n\n\n\nEl código posee una gráfica. Que hace referencia a una simulación de variables aleatorias normales. Notemos que * El elemento en azul, hace referencia a un histograma que refleja las frecuencias de los valores generados. * Mientras que la linea roja, muestra la función de densidad de una variable aleatoria estándar.\n\nEjecute y explica en pocas palabras la salida del código ex_003.py\n\nPara cambiar el vector de medias \\(\\mu\\) y la matriz \\(\\Sigma\\) hay que prestar atención en la linea donde aparece la función multivariate_normal() que de forma simple posee dos parámetros: * El vector de medias \\(\\mu = [0.5, -0.2]\\) * La matriz de covarianza \\(\\Sigma = [[2.0, 0.3], [0.3, 0.5]]\\)\n\nx = np.linspace(0, 5, 100, endpoint=False)\ny = multivariate_normal.pdf(x, mean=2.5, cov=0.5)\n\nfig1 = plt.figure()\nax = fig1.add_subplot(111)\nax.plot(x, y)\n# plt.show()\n\nx, y = np.mgrid[-5:5:.1, -5:5:.1]\npos = np.dstack((x, y))\nrv = multivariate_normal([0.1, 0.5], [[3.0, 0.3], [0.75, 1.5]])\nfig2 = plt.figure()\nax2 = fig2.add_subplot(111)\nax2.contourf(x, y, rv.pdf(pos))\n# plt.show()\n\nax = plt.figure().add_subplot(projection='3d')\nax.plot_surface(\n    x,\n    y,\n    rv.pdf(pos),\n    edgecolor='royalblue',\n    lw=0.5,\n    rstride=8,\n    cstride=8,\n    alpha=0.4\n)\nax.contour(x, y, rv.pdf(pos), zdir='z', offset=-.2, cmap='coolwarm')\nax.contour(x, y, rv.pdf(pos), zdir='x', offset=-5, cmap='coolwarm')\nax.contour(x, y, rv.pdf(pos), zdir='y', offset=5, cmap='coolwarm')\n\nax.set(\n    xlim=(-5, 5),\n    ylim=(-5, 5),\n    zlim=(-0.2, 0.2),\n    xlabel='X',\n    ylabel='Y',\n    zlabel='Z'\n)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nGenerando Normales\n\n\nimport numpy as np\nfrom scipy.stats import bernoulli\nimport matplotlib.pyplot as plt\n\nfig_01, ax_01 = plt.subplots(1, 1)\nfig_02, ax_02 = plt.subplots(1, 1)\n\np = 0.3\n\nx = np.arange(bernoulli.ppf(0.01, p), bernoulli.ppf(0.99, p))\nax_01.plot(x, bernoulli.pmf(x, p), 'bo', ms = 8, label = 'bernoulli pmf')\nax_01.vlines(x, 0, bernoulli.pmf(x, p), colors = 'b', lw = 5, alpha = 0.5)\n\nr = bernoulli.rvs(p, size = 1000)\nax_02.hist(r, bins = 200)\n\n(array([708.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n          0., 292.]),\n array([0.   , 0.005, 0.01 , 0.015, 0.02 , 0.025, 0.03 , 0.035, 0.04 ,\n        0.045, 0.05 , 0.055, 0.06 , 0.065, 0.07 , 0.075, 0.08 , 0.085,\n        0.09 , 0.095, 0.1  , 0.105, 0.11 , 0.115, 0.12 , 0.125, 0.13 ,\n        0.135, 0.14 , 0.145, 0.15 , 0.155, 0.16 , 0.165, 0.17 , 0.175,\n        0.18 , 0.185, 0.19 , 0.195, 0.2  , 0.205, 0.21 , 0.215, 0.22 ,\n        0.225, 0.23 , 0.235, 0.24 , 0.245, 0.25 , 0.255, 0.26 , 0.265,\n        0.27 , 0.275, 0.28 , 0.285, 0.29 , 0.295, 0.3  , 0.305, 0.31 ,\n        0.315, 0.32 , 0.325, 0.33 , 0.335, 0.34 , 0.345, 0.35 , 0.355,\n        0.36 , 0.365, 0.37 , 0.375, 0.38 , 0.385, 0.39 , 0.395, 0.4  ,\n        0.405, 0.41 , 0.415, 0.42 , 0.425, 0.43 , 0.435, 0.44 , 0.445,\n        0.45 , 0.455, 0.46 , 0.465, 0.47 , 0.475, 0.48 , 0.485, 0.49 ,\n        0.495, 0.5  , 0.505, 0.51 , 0.515, 0.52 , 0.525, 0.53 , 0.535,\n        0.54 , 0.545, 0.55 , 0.555, 0.56 , 0.565, 0.57 , 0.575, 0.58 ,\n        0.585, 0.59 , 0.595, 0.6  , 0.605, 0.61 , 0.615, 0.62 , 0.625,\n        0.63 , 0.635, 0.64 , 0.645, 0.65 , 0.655, 0.66 , 0.665, 0.67 ,\n        0.675, 0.68 , 0.685, 0.69 , 0.695, 0.7  , 0.705, 0.71 , 0.715,\n        0.72 , 0.725, 0.73 , 0.735, 0.74 , 0.745, 0.75 , 0.755, 0.76 ,\n        0.765, 0.77 , 0.775, 0.78 , 0.785, 0.79 , 0.795, 0.8  , 0.805,\n        0.81 , 0.815, 0.82 , 0.825, 0.83 , 0.835, 0.84 , 0.845, 0.85 ,\n        0.855, 0.86 , 0.865, 0.87 , 0.875, 0.88 , 0.885, 0.89 , 0.895,\n        0.9  , 0.905, 0.91 , 0.915, 0.92 , 0.925, 0.93 , 0.935, 0.94 ,\n        0.945, 0.95 , 0.955, 0.96 , 0.965, 0.97 , 0.975, 0.98 , 0.985,\n        0.99 , 0.995, 1.   ]),\n &lt;BarContainer object of 200 artists&gt;)\n\n\n\n\n\n\n\n\n\nfrom scipy.stats import norm\n\nfig, ax = plt.subplots(1,1)\n\nx = np.linspace(norm.ppf(0.01),norm.ppf(0.99), 100)\nax.plot(x, norm.pdf(x),'r-', lw = 5, alpha = 0.6)\n\nr = norm.rvs(size = 1000)\n\nax.hist(r, density = True, bins = 'auto', histtype = 'stepfilled', alpha = 0.2)\nax.set_xlim(x[0], x[-1])\nax.legend(loc='best', frameon = False)\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n&lt;matplotlib.legend.Legend at 0x124553cc590&gt;\n\n\n\n\n\n\nfrom mpl_toolkits.mplot3d import axes3d\nfrom scipy.stats import multivariate_normal\n\nx = np.linspace(0,5,100,endpoint = False)\ny = multivariate_normal.pdf(x , mean = 2.5, cov = 0.5)\n\nfig1 = plt.figure()\nax = fig1.add_subplot(111)\nax.plot(x,y)\n\nx, y = np.mgrid[-5:5:.1, -5:5:.1]\n\npos = np.dstack((x,y))\nrv = multivariate_normal([0.5, -0.2], [[2.0,0.3], [0.3,0.5]])\nfig2 = plt.figure()\nax2 = fig2.add_subplot(111)\nax2.contourf(x,y, rv.pdf(pos))\n\nax = plt.figure().add_subplot(projection = '3d')\nax.plot_surface(x,\n                y,\n                rv.pdf(pos),\n                edgecolor = 'royalblue',\n                lw = 0.5,\n                rstride = 8,\n                cstride = 8,\n                alpha = 0.5)\n\n&lt;mpl_toolkits.mplot3d.art3d.Poly3DCollection at 0x12455da6d10&gt;"
  },
  {
    "objectID": "Tarea2.html",
    "href": "Tarea2.html",
    "title": "3  Tarea 2",
    "section": "",
    "text": "Sea \\(X_{i}\\) v.a.i.i.d tales que\n\\[\n\\mathcal{P}\\left[X_{i}=h\\right]=\\mathcal{P}\\left[X_{i}=-h\\right]=\\dfrac{1}{2},\\forall i,\n\\]\nentonces definimos \\(Y_{n,h}\\). ::: Queremos calcular la función característica de \\(Y_{n,\\delta}\\).\n\\[\nE\\left[i\\lambda Y_{n,\\delta}\\left(t\\right)\\right],\n\\]\nAprovechando que para cada \\(X_{i}\\) son v.a.i.i.d. Entonces, tenemos lo siguiente\n\\[\\begin{align*}\nE\\left[i\\lambda Y_{n,\\delta}\\left(t\\right)\\right] & =\\left(\\cos\\left(\\lambda h\\right)\\right)^{t/\\delta}\\\\\n& =u^{t},\n\\end{align*}\\]\ndonde \\[\\begin{align*}\nu & =\\left[\\cos\\left(\\lambda h\\right)\\right]^{1/\\delta}\\\\\n\\ln\\left(u\\right) & =\\dfrac{1}{\\delta}\\ln\\left[\\cos\\left(\\lambda h\\right)\\right]\n\\end{align*}\\]\nEntonces, aproximaremos \\(\\cos\\left(\\lambda h\\right)\\) con su expansión de Taylor. \\[\n\\cos\\left(\\lambda h\\right)\\approx1-\\dfrac{\\left(\\lambda h\\right)^{2}}{2!}+\\dfrac{\\left(\\lambda h\\right)^{4}}{4!},\n\\]\nentonces \\[\\begin{align*}\n\\ln\\left(\\cos\\left(\\lambda h\\right)\\right) & \\approx\\ln\\left[1-\\dfrac{\\left(\\lambda h\\right)^{2}}{2}+\\dfrac{\\left(\\lambda h\\right)^{4}}{4!}\\right]\\\\\n& \\approx-\\dfrac{\\left(\\lambda h\\right)^{2}}{2!}+\\dfrac{\\left(\\lambda h\\right)^{4}}{4!}\n\\end{align*}\\]\nEntonces \\[\\begin{align*}\nu^{t} & \\approx\\exp\\left[\\dfrac{t}{\\delta}\\left(-\\dfrac{\\left(\\lambda h\\right)^{2}}{2!}+\\dfrac{\\left(\\lambda h\\right)^{4}}{4!}\\right)\\right],\\\\\n& \\approx\\exp\\left[-\\dfrac{t}{\\delta}\\left(\\dfrac{\\lambda^{2}h^{2}}{2}-\\dfrac{\\lambda^{4}h^{4}}{24}\\right)\\right],\n\\end{align*}\\]\nCalculando el limite \\[\n\\lim_{\\delta\\to0}E\\left[\\exp\\left(i\\lambda Y_{n,\\delta}\\left(t\\right)\\right)\\right]=\\lim_{\\delta\\to0}\\exp\\left[-t\\left(\\left[\\dfrac{h^{2}}{\\delta}\\right]\\left(\\dfrac{\\lambda^{2}}{2}-\\dfrac{\\lambda^{4}h^{2}}{24}\\right)\\right)\\right],\n\\]\nsi \\(h^{2}/\\delta\\to\\infty\\) Segun la sucesión \\(\\delta_{n}\\to0\\) tenemos limites diferentes, por lo tanto, este no existe. Ahora, usando la normalización, retomando las operaciones anteriores,\n\\[\\begin{align*}\nE\\left[\\exp\\left(i\\lambda Y_{n,\\delta}\\left(t\\right)+\\dfrac{th^{2}\\lambda^{2}}{2}\\right)\\right] & =E\\left[\\exp\\left(i\\lambda\\sum_{i=0}^{n}X_{i}+\\dfrac{th^{2}\\lambda^{2}}{2\\delta}\\right)\\right]\\\\\n& =E\\left[\\exp\\left(i\\lambda\\sum_{i=0}^{n}X_{i}\\right)\\right]\\exp\\left(\\dfrac{th^{2}\\lambda^{2}}{2\\delta}\\right)\\\\\n& =\\left(\\left[\\cos\\left(\\lambda h\\right)\\right]^{1/\\delta}\\exp\\left(\\dfrac{h^{2}\\lambda^{2}}{2\\delta}\\right)\\right)^{t},\n\\end{align*}\\]\nentonces, \\[\\begin{align*}\nv & =\\left[\\cos\\left(\\lambda h\\right)\\right]^{1/\\delta}\\exp\\left(\\dfrac{h^{2}\\lambda^{2}}{2}\\right)\\\\\n\\ln v & =\\ln\\left[\\left[\\cos\\left(\\lambda h\\right)\\right]^{1/\\delta}\\exp\\left(\\dfrac{h^{2}\\lambda^{2}}{2}\\right)\\right]\\\\\n& =\\dfrac{1}{\\delta}\\ln\\left[\\cos\\left(\\lambda h\\right)\\right]+\\dfrac{h^{2}\\lambda^{2}}{2\\delta}\\\\\n& =\\dfrac{1}{\\delta}\\left(\\ln\\left[\\cos\\left(\\lambda h\\right)\\right]+\\dfrac{h^{2}\\lambda^{2}}{2}\\right)\\\\\n& \\approx\\dfrac{1}{\\delta}\\left(\\ln\\left[1\\underbrace{-\\dfrac{\\left(\\lambda h\\right)^{2}}{2}+\\dfrac{\\left(\\lambda h\\right)^{4}}{4!}}_{x_{0}}\\right]+\\dfrac{h^{2}\\lambda^{2}}{2}\\right)\n\\end{align*}\\]\nrecordando que \\[\n\\ln\\left(1+x\\right)\\approx x-\\dfrac{x^{2}}{2},\n\\]\nentonces\n\\[\n\\ln v\\approx\\dfrac{1}{\\delta}\\left(\\left[-\\dfrac{\\left(\\lambda h\\right)^{2}}{2}+\\dfrac{\\left(\\lambda h\\right)^{4}}{4!}-\\dfrac{\\left(-\\dfrac{\\left(\\lambda h\\right)^{2}}{2}+\\dfrac{\\left(\\lambda h\\right)^{4}}{4!}\\right)^{2}}{2}\\right]+\\dfrac{h^{2}\\lambda^{2}}{2}\\right),\n\\]\nbajo la simplificación de que \\(o\\left(h^{k}\\right)\\equiv0,k\\geq4,\\) entonces \\[\\begin{align*}\n\\ln v & \\approx\\dfrac{1}{\\delta}\\left(\\dfrac{\\left(\\lambda h\\right)^{4}}{24}-\\dfrac{\\left(\\lambda h\\right)^{4}}{8}\\right)\\\\\n& \\approx\\dfrac{1}{\\delta}\\left(\\dfrac{\\left(\\lambda h\\right)^{4}}{24}-\\dfrac{3\\left(\\lambda h\\right)^{4}}{24}\\right)\\\\\nv & \\approx\\exp\\left(-\\dfrac{\\left(\\lambda h\\right)^{4}}{12\\delta}\\right)\n\\end{align*}\\]\npor lo tanto, si \\(h^{4}/\\delta\\to0\\) \\[\n\\lim_{\\delta\\to0}E\\left[\\exp\\left(i\\lambda Y_{n,\\delta}\\left(t\\right)+\\dfrac{th^{2}\\lambda^{2}}{2}\\right)\\right]=\\lim_{\\delta\\to0}\\exp\\left(-\\dfrac{\\left(\\lambda h\\right)^{4}}{12\\delta}\\right)=1\n\\]"
  },
  {
    "objectID": "Tarea3.html",
    "href": "Tarea3.html",
    "title": "4  Tarea 3",
    "section": "",
    "text": "5 Ejercicio 3.1\nSi \\(X\\thicksim N(\\mu,\\sigma)\\) entonces \\(\\left(\\dfrac{X-\\mu}{\\sigma}\\right)\\thicksim N(0,1)\\).\nCalculemos la función característica de la variable \\(\\dfrac{X-\\mu}{\\sigma}\\), \\[\\begin{eqnarray}\\label{1.1}\n\\varphi_{\\frac{X-\\mu}{\\sigma}}(t) & = & E\\left[e^{it\\left(\\frac{X-\\mu}{\\sigma}\\right)}\\right]\\nonumber\\\\\n& = & E\\left[e^{\\left(\\frac{itX}{\\sigma}-\\frac{it\\mu}{\\sigma}\\right)}\\right]\\nonumber\\\\\n& = & e^{-\\frac{it\\mu}{\\sigma}}E\\left[e^{\\left(\\frac{itX}{\\sigma}\\right)}\\right]\\nonumber\\\\\n& = & e^{-\\frac{it\\mu}{\\sigma}}\\int_{-\\infty}^{\\infty}e^{\\frac{itx}{\\sigma}}\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}}dx\\nonumber\\\\\n& = & e^{-\\frac{it\\mu}{\\sigma}}\\frac{1}{\\sigma\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{\\frac{itx}{\\sigma}}e^{-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}}dx\\nonumber\\\\\n& = & e^{-\\frac{it\\mu}{\\sigma}}\\frac{1}{\\sigma\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{\\frac{itx}{\\sigma}-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}}dx\\nonumber\\\\\n& = & e^{-\\frac{it\\mu}{\\sigma}}\\frac{1}{\\sigma\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}\\frac{(x-\\mu)^{2}-2itx\\sigma}{\\sigma^{2}}}dx\n\\end{eqnarray}\\]\nObservemos que, \\[\\begin{eqnarray}\\label{1.2}\n\\frac{(x-\\mu)^{2}-2itx\\sigma}{\\sigma^{2}} & = & \\frac{x^{2}-2x\\mu+\\mu^{2}-2itx\\sigma}{\\sigma^{2}}\\nonumber\\\\\n& = & \\frac{x^{2}}{\\sigma^{2}}-\\frac{2x\\mu}{\\sigma^{2}}+\\frac{\\mu^{2}}{\\sigma^{2}}-\\frac{2itx\\sigma}{\\sigma^{2}}\\nonumber\\\\\n& = & \\frac{x^{2}}{\\sigma^{2}}-\\frac{2x}{\\sigma}\\left(\\frac{\\mu+it\\sigma}{\\sigma^{2}}\\right)+\\frac{\\mu^{2}}{\\sigma^{2}}\\nonumber\\\\\n& = & \\left(\\frac{x}{\\sigma}-\\left(\\frac{\\mu+it\\sigma}{\\sigma}\\right)\\right)^{2}-\\left(\\frac{\\mu+it\\sigma}{\\sigma}\\right)^{2}+\\frac{\\mu^{2}}{\\sigma^{2}}\\nonumber\\\\\n& = & \\left(\\frac{x}{\\sigma}-\\left(\\frac{\\mu+it\\sigma}{\\sigma}\\right)\\right)^{2}-\\frac{2 it\\sigma\\mu}{\\sigma^{2}}-\\frac{(it\\sigma)^{2}}{\\sigma^{2}}\\nonumber\\\\\n& = & \\left(\\frac{x}{\\sigma}-\\left(\\frac{\\mu+it\\sigma}{\\sigma}\\right)\\right)^{2}-\\frac{2 it\\mu}{\\sigma}+t^{2}.\n\\end{eqnarray}\\]\nSustituyendo (\\(\\ref{1.2}\\)) en (\\(\\ref{1.1}\\)), resulta\n\\[\\begin{eqnarray}\\label{1.3}\n\\varphi_{\\frac{X-\\mu}{\\sigma}}(t) & = & e^{-\\frac{it\\mu}{\\sigma}}\\frac{1}{\\sigma\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}\\left[\\left(\\frac{x}{\\sigma}-\\left(\\frac{\\mu+it\\sigma}{\\sigma}\\right)\\right)^{2}-\\frac{2 it\\mu}{\\sigma}+t^{2}\\right]}dx\\nonumber\\\\\n& = & e^{-\\frac{it\\mu}{\\sigma}}e^{\\frac{it\\mu}{\\sigma}-\\frac{t^{2}}{2}}\\frac{1}{\\sigma\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}\\left(\\frac{x}{\\sigma}-\\left(\\frac{\\mu+it\\sigma}{\\sigma}\\right)\\right)^{2}}dx\\nonumber\\\\\n& = & e^{-\\frac{t^{2}}{2}}\\frac{1}{\\sigma\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}\\left(\\frac{x}{\\sigma}-\\left(\\frac{\\mu+it\\sigma}{\\sigma}\\right)\\right)^{2}}dx\n\\end{eqnarray}\\]\nSea \\(u=\\frac{x}{\\sigma}-\\left(\\frac{\\mu+it\\sigma}{\\sigma}\\right)\\Longrightarrow du=\\frac{1}{\\sigma}dx\\), sustituyendo esto en (\\(\\ref{1.3}\\)), resulta\n\\[\\begin{equation}\\label{1.4}\n\\varphi_{\\frac{X-\\mu}{\\sigma}}(t) = e^{-\\frac{t^{2}}{2}}\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{u^{2}}{2}}du\n\\end{equation}\\]\nde aquí se sigue que \\(u\\thicksim N(0,1)\\), entonces \\[\n\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{u^{2}}{2}}dx=1.\n\\]\nsustituyendo esto ultimo en (\\(\\ref{1.4}\\)), se tiene, \\[\n\\varphi_{\\frac{X-\\mu}{\\sigma}}(t) =e^{-\\frac{t^{2}}{2}},\n\\]\nque es la función característica de una Normal estándar, como las funciones características coinciden se concluye que \\(\\frac{X-\\mu}{\\sigma}\\thicksim N(0,1)\\).\n\n\n6 Ejercicio 3.2\nSi \\(Y\\thicksim N(0,1)\\) entonces \\(\\sigma Y+\\mu \\thicksim N(\\mu,\\sigma)\\). Calculemos la función característica de la variable \\(\\sigma Y+\\mu\\),\n\\[\\begin{eqnarray}\\label{2.1}\n\\varphi_{\\sigma Y+\\mu}(t) & = & E\\left[e^{it(\\sigma Y+\\mu)}\\right]\\nonumber\\\\\n& = & E\\left[e^{it\\sigma Y+it\\mu}\\right]\\nonumber\\\\\n& = & e^{it\\mu}E\\left[e^{it\\sigma Y}\\right]\\nonumber\\\\\n& = & e^{it\\mu}\\int_{-\\infty}^{\\infty}e^{it\\sigma y}\\frac{1}{\\sqrt{2\\pi}}e^{\\frac{-y^{2}}{2}}dy\\nonumber\\\\\n& = & e^{it\\mu}\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}(y^{2}-2yit\\sigma) }dy.\n\\end{eqnarray}\\] Observemos que, \\[\\begin{eqnarray}\\label{2.2}\ny^{2}-2yit\\sigma & = & (y-it\\sigma)^{2}-(it\\sigma)^{2}\\nonumber\\\\\n& = & (y-it\\sigma)^{2}+t^{2}\\sigma^{2}.\n\\end{eqnarray}\\] Sustituyendo, (\\(\\ref{2.2}\\)) en (\\(\\ref{2.1}\\)) resulta \\[\\begin{eqnarray}\\label{2.3}\n\\varphi_{\\sigma Y+\\mu}(t) & = & e^{it\\mu}\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}((y-it\\sigma)^{2}+t^{2}\\sigma^{2}) }dy\\nonumber\\\\\n& = & e^{it\\mu}e^{-\\frac{1}{2}t^{2}\\sigma^{2}}\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}(y-it\\sigma)^{2} }dy\n\\end{eqnarray}\\] Tomando \\(u=y-it\\sigma\\Longrightarrow du=dy\\), se tiene que\n\\[\n\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}(y-it\\sigma)^{2} }dy=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{u^{2} }{2}}du,\n\\]\nentonces \\(U\\thicksim N(0,1)\\), por lo tanto, \\[\n\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}e^{-\\frac{1}{2}(y-it\\sigma)^{2} }dy=1\n\\]\nsustituyendo esto ultimo en (\\(\\ref{2.3}\\)), resulta,\n\\[ \\varphi_{\\sigma Y+\\mu}(t)=e^{it\\mu}e^{-\\frac{1}{2}t^{2}\\sigma^{2}}=e^{it\\mu-\\frac{t^{2}\\sigma^{2}}{2}}.\n\\]\nSea \\(Z\\) una variable aleatoria tal que \\(Z\\thicksim N(\\mu,\\sigma)\\) sabemos que, \\[\n\\varphi_{Z}(t)=e^{it\\mu-\\frac{t^{2}\\sigma^{2}}{2}}.\n\\]\nDe estas dos ultimas igualdades se sigue que, \\[\n\\varphi_{Z}(t)=\\varphi_{\\sigma Y+\\mu}(t).\n\\]\nDado que tienen iguales funciones características se concluye que, \\[\n\\sigma Y+\\mu\\thicksim N(\\mu,\\sigma)\n\\]\n\n\n7 Ejercicio 3.3\nSi \\(X\\thicksim N(\\mu_{1},\\sigma_{1}^{2})\\), \\(Y\\thicksim N(\\mu_{2},\\sigma_{2}^{2})\\) además \\(X\\) y \\(Y\\) son independientes entonces \\(X+Y\\thicksim N(\\mu_{1}+\\mu_{2},\\sigma_{1}^{2}+\\sigma_{2}^{2})\\). Por definición, se tiene que,\n\\[\\begin{eqnarray}\\label{eq:3.1}\n\\varphi_{X+Y}(t) & = & E[e^{it(X+Y)}]\\nonumber\\\\\n& = & E[e^{itX}e^{itY}]\\text{ por ser independientes, del ejercicio 4}\\nonumber\\\\\n& = & E[e^{itX}]E[e^{itY}]\\nonumber\\\\\n& = &  \\varphi_{X}(t) \\varphi_{Y}(t).\n\\end{eqnarray}\\]\nPor otro lado, sea \\(Z\\) una variables aleatoria tal que, \\(Z\\thicksim N(\\mu_{1}+\\mu_{2},\\sigma_{1}^{2}+\\sigma_{2}^{2})\\), sabemos que la función característica de \\(Z\\), esta dada por,\n\\[\\begin{align*}\n\\varphi_{Z}(t) & = & e^{it(\\mu_{1}+\\mu_{2})-\\frac{t^{2}}{2}(\\sigma_{1}^{2}+\\sigma_{2}^{2})}\\nonumber\\\\\n& = & e^{it\\mu_{1}-\\frac{t^{2}\\sigma_{1}^{2}}{2}+it\\mu_{2}-\\frac{t^{2}\\sigma_{2}^{2}}{2}}\\nonumber\\\\\n& = & e^{it\\mu_{1}-\\frac{t^{2}\\sigma_{1}^{2}}{2}}e^{it\\mu_{2}-\\frac{t^{2}\\sigma_{2}^{2}}{2}}\\\\\n& = &  \\varphi_{X}(t) \\varphi_{Y}(t),\n\\end{align*}\\]\nentonces, de esta ultima igualdad y de \\(\\eqref{3.1}\\) se sigue que, \\[\n\\varphi_{Z}(t)= \\varphi_{X+Y}(t).\n\\]\nComo las funciones características coinciden se sigue que, \\(X+Y\\thicksim N(\\mu_{1}+\\mu_{2},\\sigma_{1}^{2}+\\sigma_{2}^{2})\\).\n\n\n8 Ejercicio 3.4\nSi \\(X,Y\\) son variables normales independientes. Entonces \\(E\\left[XY\\right]=E\\left[X\\right]E\\left[Y\\right]\\).\nRecrodemos que \\[\nE\\left[XY\\right]=\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}xyf_{XY}\\left(x,y\\right)\\mathrm{d}x\\mathrm{d}y\n\\]\nComo \\(X,Y\\) son independientes \\[\nf_{XY}\\left(x,y\\right)=f_{X}\\left(x\\right)f_{Y}\\left(y\\right)\n\\]\nEntonces\n\\[\\begin{align*}\nE\\left[XY\\right] & =\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}xyf_{XY}\\left(x,y\\right)\\mathrm{d}x\\mathrm{d}y\\\\\n& =\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}xyf_{X}\\left(x\\right)f_{y}\\left(y\\right)\\mathrm{d}x\\mathrm{d}y\\\\\n& =\\left[\\int_{-\\infty}^{\\infty}xf_{X}\\left(x\\right)\\mathrm{d}x\\right]\\left[\\int_{-\\infty}^{\\infty}yf_{y}\\left(y\\right)\\mathrm{d}y\\right]\\\\\n& =E\\left[X\\right]E\\left[Y\\right]\n\\end{align*}\\]\n\n\n9 Ejercicio 3.5\nPor Demostrar \\[\n\\mathcal{P}\\left[\\left|X-\\mu\\right|\\geq\\epsilon\\right]\\leq\\dfrac{\\text{Var}\\left[X\\right]}{\\epsilon^{2}}\n\\]\nPor la desigualdad de Chebysev, para \\(X\\) una variable aleatoria. \\[\n\\mathcal{P}\\left[X\\geq\\epsilon\\right]\\leq\\dfrac{E\\left[X\\right]}{\\epsilon}\n\\]\nEntonces, sea \\(Y=\\left|X-\\mu\\right|,\\mu=E\\left[X\\right]\\)\n\\[\\begin{align*}\n\\mathcal{P}\\left[\\left|X-\\mu\\right|\\geq\\epsilon\\right] & =\\mathcal{P}\\left[\\left|X-\\mu\\right|^{2}\\geq\\epsilon^{2}\\right]\\\\\n& \\leq\\dfrac{E\\left[\\left(X-\\mu\\right)^{2}\\right]}{\\epsilon^{2}}=\\dfrac{\\text{Var}\\left[X\\right]}{\\epsilon^{2}}\n\\end{align*}\\]\n\n\n10 Ejercicio 3.6 (Ley de los grandes números)\nPor demostrar\nSean \\(X_{1},X_{2},\\ldots,X_{n}\\) variables aleatorias independienes con esperanza finita \\(\\mu=E\\left[X_{j}\\right]\\) y varianza infinita. \\(\\sigma^{2}=\\text{Var}\\left(X_{j}\\right)\\). Sean \\(S_{n}=X_{1}+X_{2}+\\ldots+X_{n}\\). Entonces para cada \\(\\epsilon&gt;0\\).\n\\[\n\\mathcal{P}\\left[\\left|\\dfrac{S_{n}}{n}-\\mu\\right|\\geq\\epsilon\\right]\\to0\n\\]\nNotemos que \\[\\begin{align*}\n\\text{Var}\\left[\\dfrac{S_{n}}{n}-\\mu\\right] & =\\dfrac{1}{n^{2}}\\text{Var}\\left(S_{n}\\right)=\\dfrac{1}{n^{2}}\\sum_{i=1}^{n}\\text{Var}\\left(X_{i}\\right)\\\\\n& =\\dfrac{\\sigma^{2}}{n}\n\\end{align*}\\]\nEntonces, por el Teorema de Chebysev \\[\n\\mathcal{P}\\left[\\left|\\dfrac{S_{n}}{n}-\\mu\\right|\\geq\\epsilon\\right]\\leq\\dfrac{\\sigma^{2}}{n\\epsilon},\n\\]\nnotemos que para \\(n\\to\\infty\\)\n\\[\n\\dfrac{\\sigma^{2}}{n\\epsilon}\\to0.\n\\]\nEntonces \\[\n\\mathcal{P}\\left[\\left|\\dfrac{S_{n}}{n}-\\mu\\right|\\geq\\epsilon\\right]\\to0\n\\]\n\n\n11 Ejercicio 3.7 (Teorema de Limite Central)\nSea \\(\\left\\{ X_{i}\\right\\} _{i=1}^{\\infty}\\) una secuencia de v.a.i.id con media \\(a\\) y varianza \\(b^{2}\\). Entonces para doo \\(\\alpha,\\beta\\in\\mathbb{R}\\), con \\(\\alpha&lt;\\beta\\), entonces\n\\[\n\\mathcal{P}\\left(\\lim_{M\\to\\infty}\\alpha\\le\\dfrac{{\\displaystyle \\sum_{i=1}^{M}}X_{i}-Ma}{\\sqrt{M}b}\\leq\\beta\\right)=\\dfrac{1}{\\sqrt{2\\pi}}\\int_{\\alpha}^{\\beta}\\exp\\left(-\\dfrac{1}{2}x^{2}\\right)\\mathrm{d}x\n\\]\nSea \\[\nY_{M}=\\dfrac{{\\displaystyle \\sum_{i=1}^{M}}\\left[X_{i}-a\\right]}{\\sqrt{M}b},\n\\]\nDefinamos \\[\n\\overline{S_{M}}={\\displaystyle \\sum_{i=1}^{M}}\\left[X_{i}-a\\right],\n\\]\nentonces \\[\nY_{M}=\\dfrac{S_{M}}{\\sqrt{M}b}\n\\]\ndemostraremos que la función generadora de momentos \\(\\varphi_{M}\\to\\varphi\\) donde \\(\\varphi_{m}=\\varphi_{Y_{M}}\\) y \\(\\varphi\\) es función generadora de momentos de la distribución normal estandar.\nAhora, \\[\\begin{align*}\n\\varphi_{M}\\left(t\\right) & =E\\left[\\exp\\left(t\\dfrac{S_{M}}{\\sqrt{Mb}}\\right)\\right]\\\\\n& =\\varphi_{SM}\\left(\\dfrac{t}{\\sqrt{M}b}\\right)\\\\\nX_{i}\\text{ v.a.i.i.d}\\Rightarrow & =\\left[\\varphi_{\\left(X_{1}-a\\right)}\\left(\\dfrac{t}{\\sqrt{M}b}\\right)\\right]^{M}\\\\\n& =\\left[E\\left[\\exp\\left(\\dfrac{t}{b\\sqrt{M}}\\left(X_{1}-a\\right)\\right)\\right]\\right]\n\\end{align*}\\]\nRecordando la serie de Taylor \\[\\begin{align*}\n\\varphi_{M}\\left(t\\right) & =\\left[\\sum_{i=0}^{\\infty}\\dfrac{E\\left[\\left(\\dfrac{t}{b\\sqrt{M}}\\left(X_{1}-a\\right)\\right)^{i}\\right]}{i!}\\right]^{M}\\\\\n& =\\left[1+\\dfrac{1}{2}\\left(\\dfrac{t}{b\\sqrt{M}}\\right)^{2}E\\left[\\left(X_{1}-a\\right)^{2}\\right]+\\epsilon\\left(3\\right)\\right]^{M}\\\\\n& =\\left[1+\\dfrac{1}{M}\\dfrac{t^{2}}{2}+\\epsilon\\left(3\\right)\\right]^{M},\n\\end{align*}\\]\ndonde \\[\\begin{align*}\n\\epsilon\\left(3\\right) & =\\sum_{i=3}^{\\infty}\\dfrac{E\\left[\\left(\\dfrac{t}{b\\sqrt{M}}\\left(X_{1}-a\\right)\\right)^{i}\\right]}{i!},\n\\end{align*}\\]\nSea \\(s=\\dfrac{t}{b\\sqrt{M}},\\) entonces \\(s\\to0,t\\to0\\) \\[\n\\epsilon\\left(3\\right)=\\sum_{i=3}^{\\infty}\\dfrac{E\\left[\\left(X_{1}-a\\right)^{i}\\right]s^{i}}{i!}\n\\]\nNotemos que, si \\(\\varphi_{1}\\) existe. Entonces \\[\n\\dfrac{\\epsilon\\left(3\\right)}{s^{2}}=\\sum_{i=3}^{\\infty}\\dfrac{E\\left[\\left(X_{1}-a\\right)^{i}\\right]s^{i-2}}{i!}\\to0,s\\to0.\n\\]\nAdemás \\(s\\to 0\\) cuando \\(M\\to\\infty\\). \\[\n\\Rightarrow\\varphi_{M}\\left(t\\right)=\\left[1+\\dfrac{1}{M}\\left[\\dfrac{t^{2}}{2}+M\\epsilon\\left(3\\right)\\right]\\right]^{M},\n\\]\nEntonces \\(\\epsilon\\left(3\\right)s^{-2}=Me\\left(3\\right)b^{2}t^{-2}\\to0\\). Como \\(b,t\\) estan fijas. \\[\nM\\epsilon\\left(3\\right)\\to0,M\\to\\infty,\n\\]\npor lo tanto \\[\\begin{align*}\n\\dfrac{t^{2}}{2}+M\\epsilon\\left(3\\right) & \\to\\dfrac{t^{2}}{2},M\\to\\infty\\\\\n\\left[1+\\dfrac{1}{M}\\left[\\dfrac{t^{2}}{2}+M\\epsilon\\left(3\\right)\\right]\\right]^{M} & \\to\\exp\\left(t^{2}\\right),M\\to\\infty\\\\\n\\lim_{M\\to\\infty}\\varphi_{M}\\left(t\\right) & =\\exp\\left(t^{2}\\right)=\\varphi\\left(t\\right),\n\\end{align*}\\]\nla función generadora de momentos de la distribución normal estándar. Por lo tanto \\[\nF_{M}\\left(x\\right)\\to F_{N\\left(0,1\\right)}\\left(x\\right)\n\\]\n\\[\\begin{align*}\nF_{M}\\left(b\\right)-F_{M}\\left(a\\right) & \\to F_{N}\\left(b\\right)-F_{N}\\left(a\\right)\\\\\n\\mathcal{P}\\left(\\lim_{M\\to\\infty}\\alpha\\le\\dfrac{{\\displaystyle \\sum_{i=1}^{M}}X_{i}-Ma}{\\sqrt{M}b}\\leq\\beta\\right) & =\\dfrac{1}{\\sqrt{2\\pi}}\\int_{\\alpha}^{\\beta}\\exp\\left(-\\dfrac{1}{2}x^{2}\\right)\\mathrm{d}x\n\\end{align*}\\]\n\n\n12 Ejercicio 3.8\nSea \\(\\left\\{ X_{i}\\right\\} _{i=1}^{\\infty}\\) una sucesión de v.a.i.i.d con media \\(a\\). Entonces \\[\n\\mathcal{P}\\left[\\lim_{M\\to\\infty}\\dfrac{1}{M}\\sum_{i=1}^{M}X_{i}=a\\right]=1.\n\\]\nEsto es similar a decir que \\[\n\\lim_{M\\to\\infty}\\dfrac{1}{M}\\sum_{i=1}^{M}X_{i}\\stackrel{\\text{c.s}}{=}a\n\\]\nSin perdida de generalidad, diremos que \\(X_{i}\\geq0,\\forall i\\). Definamos \\[\nY_{n}=X_{n}I_{\\left[\\left|X_{n}\\right|\\leq n\\right]},Q_{n}=\\sum_{i=1}^{n}Y_{i}\n\\]\nPor la desigualdad de \\[\\begin{align*}\n\\sum_{n=1}^{\\infty}\\mathcal{P}\\left[\\left|\\dfrac{Q_{n}-E\\left[Q_{n}\\right]}{n}\\right|\\geq\\epsilon\\right] & \\leq\\sum_{n=1}^{\\infty}\\dfrac{\\text{Var}\\left(Q_{n}\\right)}{\\epsilon^{2}n^{2}}=\\sum_{n=1}^{\\infty}\\dfrac{1}{\\epsilon^{2}n^{2}}\\sum_{i=1}^{n}\\text{Var}\\left(Y_{i}\\right)\\\\\n& \\leq\\sum_{n=1}^{\\infty}\\dfrac{E\\left(Y_{n}^{2}\\right)}{\\epsilon^{2}n^{2}}=\\sum_{n=1}^{\\infty}\\dfrac{1}{\\epsilon^{2}n^{2}}\\int_{0}^{n}x^{2}\\mathrm{d}F\\\\\n& \\leq\\sum_{n=1}^{\\infty}\\dfrac{1}{\\epsilon^{2}}\\int_{0}^{n}x\\mathrm{d}F&lt;\\infty,\n\\end{align*}\\]\ndonde \\(F\\) es la función de distribución de \\(X_{i}\\). Luego \\[\nE\\left[X_{1}\\right]=\\lim_{n\\to\\infty}\\int_{0}^{n}x\\mathrm{d}F=\\lim_{n\\to\\infty}E\\left[Y_{n}\\right]=\\lim_{n\\to\\infty}\\dfrac{E\\left[Q_{n}\\right]}{n}.\n\\]\nEntonces, por el Lema de Borel Canteli. \\(\\mathcal{\\mathcal{P}}\\left[\\limsup\\left(\\left|\\dfrac{Q_{n}-E\\left[Q_{n}\\right]}{n}\\right|\\geq\\epsilon\\right)\\right]=0\\)\n\\[\n\\lim_{n\\to\\infty}\\dfrac{Q_{n}}{n}=E\\left[X_{1}\\right],\\text{c.s}\n\\]\nAhora, calcularemos la siguiente probabilidad \\[\n\\sum_{i=1}^{\\infty}\\mathcal{P}\\left[X_{i}\\neq Y_{i}\\right]=\\sum_{i=1}^{\\infty}\\mathcal{P}\\left[X_{i}&gt;n\\right]\n\\]\ncomo \\(E\\left[X_{i}\\right]&lt;\\infty\\) y \\(X_{i}\\) son v.a.i.i.d.\n\\[\n\\sum_{i=1}^{\\infty}\\mathcal{P}\\left[X_{i}&gt;n\\right]\\leq E\\left[X_{1}\\right]&lt;\\infty\n\\]\nDe nuevo, por el Lema de Borel Cantelli. \\[\n\\mathcal{P}\\left[\\limsup\\left[X_{i}\\neq Y_{i}\\right]\\right]=0,\\forall i\n\\]\nEntonces \\[\\begin{align*}\nX_{i} & =Y_{i},\\text{c.s}\\\\\n\\Rightarrow & \\dfrac{1}{M}\\sum_{i=1}^{M}X_{i}\\to E\\left[X_{1}\\right]=\\mu.\\text{ c.s}\n\\end{align*}\\]"
  },
  {
    "objectID": "Tarea4.html",
    "href": "Tarea4.html",
    "title": "5  Tarea 4",
    "section": "",
    "text": "Sea \\(W(t)\\) un movimiento Browniano estándar en \\([0,T]\\). Pruebe que para cualquier \\(c&gt;0\\) fijo, \\[\nV(t) = \\dfrac{1}{c} W(c^2 t)\n\\]\nes un movimiento Browniano sobre \\([0,T]\\).\n\n\n5.0.1 Demostración\nDemostraremos que \\(V\\) cumple las propiedades del movimiento Browniano.\n\n5.0.1.1 Propiedad 1\nEs claro que \\(V(0) = \\dfrac{1}{c} W (c^2 0)=0\\).\n\n\n5.0.1.2 Propiedad 2 (Incrementos Independientes)\nSean \\(s&lt;t&lt;u&lt;v\\) tenemos que \\[\nE[\\left(V(t)-V(s)\\right)\\left(V(v)-V(u)\\right)]=\\dfrac{1}{c^2}E[\\left(W(c^2 t)-W(c^2 s)\\right)\\left(W(c^2 v)-W(c^2 u)\\right)]\n\\]\nComo el browniano tiene incrementos independientes. \\[\\begin{align*}\n\\dfrac{1}{c^{2}}E\\left[\\left(W(c^{2}t)-W(c^{2}s)\\right)\\left(W(c^{2}v)-W(c^{2}u)\\right)\\right] & =\\dfrac{1}{c^{2}}E\\left[\\left(W(c^{2}t)-W(c^{2}s)\\right)\\right]E\\left[\\left(W(c^{2}v)-W(c^{2}u)\\right)\\right]\\\\\n& =0\n\\end{align*}\\]\nEntonces \\(V\\) tiene incrementos independientes.\n\n\n5.0.1.3 Propiedad 3 (Incrementos estacionarios)\nConsidere \\(s&lt;t\\). \\[\nV(t)-V(s)=\\dfrac{1}{c}\\left[W(c^2 t) - W(c^2 s)\\right]\n\\]\nPor propiedades del movimiento Browniano.\n\\[\\begin{align*}\nE\\left[V(t)-V(s)\\right] & =\\dfrac{1}{c}E\\left[W(c^{2}t)-W(c^{2}s)\\right]=0\\\\\n\\text{Var}\\left[V(t)-V(s)\\right] & =\\dfrac{1}{c^{2}}\\text{Var}\\left[W(c^{2}t)-W(c^{2}s)\\right]=\\dfrac{1}{c^{2}}\\left(c^{2}\\left(t-s\\right)\\right)=t-s\n\\end{align*}\\]\nEntonces \\(V\\) tiene incrementos estacionarios.\n\n\n\n5.0.2 Por lo tanto, \\(V\\) es un movimiento browniano.\n\nHacer un script para ilustrar la propiedad de escalado del movimiento Browniano para el caso de \\(c = \\dfrac{1}{5}\\). Estar seguro que usa el mismo camino browniano discretizado en cada subplot.\n\nEl código, se encuentra en hw4_p2.py. Pero aquí se muestran los resultados.\n\n# Importamos las librerias necesarias \nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Comenzaremos simulando el movimiento browniano fuerte.\n\nprng = np.random.RandomState(10)  # Fijamos la semilla. \n\nt_final = 1  # Extremo derecho del intervalo [0,T]\nn_points = 64  # No olvidemos que aquí se incluye el 0 y t.\ndt = 1 / (n_points - 1)  # Ajuste al delta t\n\ndw = np.sqrt(dt) * prng.standard_normal(n_points - 1)  # Calculamos los incrementos.\nw = np.concatenate(([0], dw.cumsum()))\n\ntime = np.linspace(0, t_final, n_points)  # Vector de tiempo.\n\n# plt.plot(time, w)        Graficamos el browniano base.\n# plt.show()\n\n\"\"\"\nAhora, comenzamos con el browniano escalado. \n\"\"\"\nc = 0.2  # 1/5\n\n\n\"\"\"\nEsto tiene dos interpretaciones. \nSin embargo, para este ejercicio debemos partir de una trayectoria dada, entonces haremos la transformación. \n\"\"\"\n\nc_time = c**2 * time  # Transformamos el intervalo del tiempo\nc_w = c**(-1) * w  # Escalamos el browniano. \n\nprint(\"El valor de c es \",c)\nfig, cbrown = plt.subplots(2)\ncbrown[0].plot(time, w)\ncbrown[1].plot(c_time, c_w)\ncbrown[0].set_title('Movimiento browniano')\ncbrown[1].set_title('Movimiento browniano escalado')\nplt.show()\n\nEl valor de c es  0.2\n\n\n\n\n\n\nModifique el script half_brownian_refinement.py encapsulando el código en una función. Esta función deberá recibir el extremo derecho del intervalo \\([0, T]\\) y el número de incrementos \\(N\\) de un camino browniano base. El propósito es calcular los incrementos de relleno de una refinamiento con \\(2N\\) incrementos.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nprng = np.random.RandomState(10)\n\ndef refined_brownian_2n(T,L):\n    dt = T / L\n    W = np.zeros(L + 1)\n    W_refined = np.zeros(2 * L + 1)\n    xi = np.sqrt(dt) * prng.normal(size=L)\n    xi_half = np.sqrt(0.5 * dt) * prng.normal(size=L)\n    W[1:] = xi.cumsum()\n    W_ = np.roll(W, -1)\n    W_half = 0.5 * (W + W_)\n    W_half = np.delete(W_half, -1) + xi_half\n    W_refined[1::2] = W_half\n    W_refined[2::2] = W[1:]\n    t = np.arange(0, T + dt, dt)\n    t_half = np.arange(0, T + 0.5 * dt, 0.5 * dt)\n    return t, t_half, W, W_refined\n\n\nEn un script separado, incluya la función de arriba y grafique una figura con la trayectoria del browniano con 100 incrementos y muestre su refinamiento correspondiente.\n\n\ntime, bi_time, w, bi_w = refined_brownian_2n(1,100)\n\nplt.plot(time, w, 'b-+')\nplt.plot(\n    bi_time,\n    bi_w,\n    'ro--',\n    alpha=0.5\n)\nplt.show()"
  },
  {
    "objectID": "Tarea5.html",
    "href": "Tarea5.html",
    "title": "6  Tarea 5",
    "section": "",
    "text": "Exercise 6.1 Muestre que el movimiento Browniano satisface \\[\nE\\left[\\left|W\\left(t\\right)-W\\left(s\\right)\\right|^{2}\\right]=\\left|t-s\\right|\n\\]\n\nSi \\(t&gt;s\\). \\[\\begin{align*}\nE\\left[\\left|W\\left(t\\right)-W\\left(s\\right)\\right|^{2}\\right] & =E\\left[\\left(W\\left(t\\right)-W\\left(s\\right)\\right)^{2}\\right]\\\\\n& =t-s,\n\\end{align*}\\]\nmientras que si \\(t\\leq s\\). \\[\\begin{align*}\nE\\left[\\left(W\\left(t\\right)-W\\left(s\\right)\\right)^{2}\\right] & =E\\left[\\left(W\\left(s\\right)-W\\left(t\\right)\\right)^{2}\\right]\\\\\n& =s-t,\n\\end{align*}\\]\npor lo tanto \\[\nE\\left[\\left|W\\left(t\\right)-W\\left(s\\right)\\right|^{2}\\right]=\\left|t-s\\right|\n\\]\n\nExercise 6.2 Dado \\(W\\left(t_{i}\\right)\\) y \\(W\\left(t_{i+1}\\right)\\), muestre que la variable aleatoria \\[\nW\\left(t_{i+\\frac{1}{2}}\\right):=\\dfrac{1}{2}\\left[W\\left(t_{i}\\right)+W\\left(t_{i+1}\\right)\\right]+\\dfrac{1}{2}\\sqrt{\\Delta t}\\xi,\\xi\\sim N\\left(0,1\\right)\n\\] es un movimiento Browniano.\n\n\n6.0.0.0.1 Es claro que al ser un refinamiento del movimiento browniano.\n\\[\nW\\left(0\\right)=0\n\\]\n\n\n6.0.0.0.2 \\(C_{2}\\). Notemos que\n\\[\nW_{i+\\frac{i}{2}}-W_{i}=\\dfrac{1}{2}\\left[W\\left(t_{i+1}\\right)-W\\left(t_{i}\\right)\\right]+\\dfrac{1}{2}\\sqrt{\\Delta t}\\xi,\n\\]\nSabemos que la combinación lineal de normales es una nornal. Luego, partiendo que \\(t_{i+1}-t_{i}=\\Delta t\\). \\[\\begin{align*}\nE\\left[W_{i+\\frac{i}{2}}-W_{i}\\right] & =0,\\\\\n\\text{Var}\\left[W_{i+\\frac{i}{2}}-W_{i}\\right] & =\\dfrac{1}{4}\\Delta t+\\dfrac{1}{4}\\Delta t=\\dfrac{1}{2}\\Delta t,\n\\end{align*}\\]\nPor lo tanto \\(W_{i+\\frac{1}{2}}-W_{i}\\sim N\\left(0,\\dfrac{\\Delta t}{2}\\right)\\).\n\n\n6.0.0.0.3 Calculamos la esperanza.\n\\[\nE\\left[\\left(W_{i+1}-W_{i+\\frac{1}{2}}\\right)\\left(W_{j+1}-W_{j+\\frac{1}{2}}\\right)\\right]=E\\left[\\left(\\dfrac{1}{2}\\left[W\\left(t_{i+1}\\right)-W\\left(t_{i}\\right)\\right]+\\sqrt{\\dfrac{\\Delta t}{4}}\\xi\\right)\\dfrac{1}{2}\\left[W\\left(t_{j+1}\\right)-W\\left(t_{j}\\right)\\right]+\\sqrt{\\dfrac{\\Delta t}{4}}\\xi\\right],\n\\]\ndefina \\(dW_{i}=W_{i+1}-W_{i},\\)\n\\[\\begin{align*}\nE\\left[\\left(W_{i+1}-W_{i+\\frac{1}{2}}\\right)\\left(W_{j+1}-W_{j+\\frac{1}{2}}\\right)\\right] & =E\\left[\\left(\\dfrac{1}{2}dW_{i}+\\sqrt{\\dfrac{\\Delta t}{4}}\\xi\\right)\\left(\\dfrac{1}{2}dW_{j}+\\sqrt{\\dfrac{\\Delta t}{4}}\\xi\\right)\\right]\\\\\n& =E\\left[\\dfrac{1}{4}dW_{i}dW_{j}+\\dfrac{1}{2}dW_{i}\\sqrt{\\dfrac{\\Delta t}{4}}\\xi+\\dfrac{1}{2}dW_{j}\\sqrt{\\dfrac{\\Delta t}{4}}\\xi+\\left(\\sqrt{\\dfrac{\\Delta t}{4}}\\xi\\right)^{2}\\right]\\\\\ndW_{i},dW_{j}\\text{ son independientes} & =\\left(E\\left[\\dfrac{1}{2}dW_{i}\\right]\\right)\\left(E\\left[\\dfrac{1}{2}dW_{j}\\right]\\right)+E\\left[\\dfrac{1}{2}dW_{i}\\right]\\sqrt{\\dfrac{\\Delta t}{4}}E\\left[\\xi\\right]\\\\\n& +E\\left[\\dfrac{1}{2}dW_{j}\\right]\\sqrt{\\dfrac{\\Delta t}{4}}E\\left[\\xi\\right]+\\dfrac{\\Delta t}{4}\\left(E\\left[\\xi\\right]\\right)^{2}\\\\\n& =\\left(E\\left[\\dfrac{1}{2}dW_{i}\\right]\\right)\\left(E\\left[\\dfrac{1}{2}dW_{j}+\\sqrt{\\dfrac{\\Delta t}{4}}\\xi\\right]\\right)+E\\left[\\dfrac{1}{2}dW_{j}\\right]\\sqrt{\\dfrac{\\Delta t}{4}}E\\left[\\xi\\right]+\\dfrac{\\Delta t}{4}\\left(E\\left[\\xi\\right]\\right)^{2}\\\\\n& =\\left(E\\left[\\dfrac{1}{2}dW_{i}\\right]\\right)\\left(E\\left[\\left(W_{j+1}-W_{j+\\frac{1}{2}}\\right)\\right]\\right)+E\\left[\\dfrac{1}{2}dW_{j}\\right]\\sqrt{\\dfrac{\\Delta t}{4}}E\\left[\\xi\\right]+\\dfrac{\\Delta t}{4}\\left(E\\left[\\xi\\right]\\right)^{2}\\\\\n& =\\left(E\\left[\\dfrac{1}{2}dW_{i}\\right]\\right)\\left(E\\left[\\left(W_{j+1}-W_{j+\\frac{1}{2}}\\right)\\right]\\right)+E\\left[\\dfrac{1}{2}dW_{j}+\\sqrt{\\dfrac{\\Delta t}{4}}\\xi\\right]\\sqrt{\\dfrac{\\Delta t}{4}}E\\left[\\xi\\right]\\\\\n& =\\left(E\\left[\\dfrac{1}{2}dW_{i}\\right]\\right)\\left(E\\left[\\left(W_{j+1}-W_{j+\\frac{1}{2}}\\right)\\right]\\right)+E\\left[\\left(W_{j+1}-W_{j+\\frac{1}{2}}\\right)\\right]\\sqrt{\\dfrac{\\Delta t}{4}}E\\left[\\xi\\right]\\\\\n& =E\\left[\\left(W_{i+1}-W_{i+\\frac{1}{2}}\\right)\\right]E\\left[\\left(W_{j+1}-W_{j+\\frac{1}{2}}\\right)\\right],\n\\end{align*}\\] teniendo asi, que los incrementos son independientes.\n\nExercise 6.3 Generalice la fórmula del ejercicio anterior para en el caso donde, \\(W\\left(t_{i}\\right),W\\left(t_{i+1}\\right),\\) y \\(\\alpha\\in\\left(0,1\\right)\\) el valor \\[\nW\\left(t_{i}+\\alpha\\Delta t\\right),\n\\]\nes un movimiento Browniano.\n\nNotemos que \\[\nt_{i}+\\alpha\\Delta t+\\left(1-\\alpha\\right)\\Delta t=t_{i+1},\n\\] entonces vamos a definir \\[\\begin{align*}\nW_{i+\\alpha} & =W\\left(t_{i}+\\alpha\\Delta t\\right)\\\\\n& =\\left(1-\\alpha\\right)W_{i}+\\alpha W_{i+1}+Y,\n\\end{align*}\\] donde \\(Y\\) será una v.a independiente de \\(W\\left(t\\right)\\). Entonces \\[\\begin{align*}\nW_{i+\\alpha}-W_{i} & =\\left(1-\\alpha\\right)W_{i}+\\alpha W_{i+1}+Y-W_{i}\\\\\n& =\\alpha\\left(W_{i+1}-W_{i}\\right)+Y\\\\\n& =\\alpha\\left(W_{i+1}-W_{i}\\right)+Y.\n\\end{align*}\\]\nEntonces \\[\nE\\left[W_{i+\\alpha}-W_{i}\\right]=E\\left[Y\\right],\n\\]\npor lo tanto, \\(E\\left[Y\\right]\\) tiene que ser cero. Luego \\[\n\\text{Var}\\left[W_{i+\\alpha}-W_{i}\\right]=\\alpha^{2}\\Delta t+\\text{Var}\\left[Y\\right],\n\\]\nnotemos que \\[\n\\left(i+\\alpha\\right)\\Delta t-i\\Delta t=\\alpha\\Delta t,\n\\]\npor lo tanto tendría que cumplirse \\(\\text{Var}\\left[W_{i+\\alpha}-W_{i}\\right]=\\alpha\\Delta t\\).\n\\[\n\\alpha^{2}\\Delta t+\\text{Var}\\left[Y\\right]=\\alpha\\Delta t,\n\\]\nentonces \\[\n\\text{Var}\\left[Y\\right]=\\Delta t\\left(\\alpha-\\alpha^{2}\\right),\n\\]\ncomo \\(Y=\\sqrt{\\alpha\\left(1-\\alpha\\right)}\\xi,\\xi\\sim N\\left(0,1\\right)\\). Como este es un refinamiento del browniano, entonces se cumple \\(C1\\). \\[\nW\\left(0\\right)=0.\n\\]\nConseguimos \\(C2\\) por construcción y de forma análoga tenemos la independiencia de los incrementos. \\[\nE\\left[\\left(W_{i+\\alpha}-W_{i}\\right)\\left(W_{j+\\alpha}-W_{j}\\right)\\right]=E\\left[W_{i+\\alpha}-W_{i}\\right]E\\left[W_{j+\\alpha}-W_{j}\\right].\n\\]\n\nExercise 6.4 Suponga que \\(X\\sim N\\left(0,1\\right)\\). Sabemos que \\(E\\left[X\\right]=0\\) y \\(E\\left[X^{2}\\right]=3\\). Luego, de la definición, el \\(p-\\) ésimo momento satisface \\[\nE\\left[X^{p}\\right]=\\dfrac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}x^{p}\\exp\\left(-\\frac{x^{2}}{2}\\right)\\mathrm{D}x.\n\\]\nUsando la relación, muestre que \\(E\\left[X^{3}\\right]=0\\) y \\(E\\left[X^{4}\\right]=1\\).\nEntonces deduzca el incremento Browniano, \\[\n\\Delta W_{i}=W\\left(t_{i+1}\\right)-W\\left(t_{i}\\right),\n\\]\nsatisface \\(E\\left[\\Delta W_{i}^{3}\\right]=0,E\\left[\\Delta W_{i}^{4}\\right]=3\\left(\\Delta t\\right)^{2}\\). Entonces encuentre una expresión para \\(E\\left[X^{p}\\right]\\) para un entero positivo \\(p\\).\\ Pista: Tu puedes usar el dato que \\(\\int_{-\\infty}^{\\infty}\\exp\\left(-\\dfrac{x^{2}}{2}\\right)=\\sqrt{2\\pi}.\\)\n\nConsidere la fórmula. \\[\nE\\left[X^{p}\\right]=\\dfrac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}x^{p}\\exp\\left(-\\frac{x^{2}}{2}\\right)\\mathrm{D}x.\n\\]\nPartiendo la integral, \\[\n\\dfrac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}x^{p}\\exp\\left(-\\frac{x^{2}}{2}\\right)\\mathrm{D}x=\\underbrace{\\dfrac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{0}x^{p}\\exp\\left(-\\frac{x^{2}}{2}\\right)\\mathrm{D}x}_{I_{1}}+\\underbrace{\\dfrac{1}{\\sqrt{2\\pi}}\\int_{0}^{\\infty}x^{p}\\exp\\left(-\\frac{x^{2}}{2}\\right)\\mathrm{D}x}_{I_{2}}\n\\]\nEntonces \\[\nI_{1}=\\dfrac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{0}x^{p}\\exp\\left(-\\dfrac{x^{2}}{2}\\right)\\mathrm{D}x,\n\\]\nhagamos el cambio de variable. \\(y=-x,\\) tenemos que \\[\\begin{align*}\nI_{1} & =\\dfrac{1}{\\sqrt{2\\pi}}\\int_{\\infty}^{0}-\\left(-y\\right)^{p}\\exp\\left(-\\dfrac{y^{2}}{2}\\right)\\mathrm{D}y\\\\\n& =\\dfrac{\\left(-1\\right)^{p}}{\\sqrt{2\\pi}}\\int_{0}^{\\infty}y^{p}\\exp\\left(-\\dfrac{y^{2}}{2}\\right)\\mathrm{D}y=I_{2}\\left(-1\\right)^{p},\n\\end{align*}\\] entonces \\[\nE\\left[X^{p}\\right]=\\left(1+\\left(-1\\right)^{p}\\right)I_{2},\n\\]\nde aqui tenemos, que si \\(p\\) es impar \\(E\\left[X^{p}\\right]=0,\\) entonces si \\(p\\) es par \\[\nE\\left[X^{p}\\right]=2I_{2},\n\\]\nentonces, nos concentraremos en \\[\nE\\left[X^{p}\\right]=\\dfrac{2}{\\sqrt{2\\pi}}\\int_{0}^{\\infty}x^{p}\\exp\\left(-\\frac{x^{2}}{2}\\right)\\mathrm{D}x,p=2k,k\\in\\mathbb{N}\n\\]\nConsidere \\(y=\\dfrac{x^{2}}{2},\\mathrm{D}y=x\\mathrm{D}x\\).\n\\[\nE\\left[X^{p}\\right]=\\dfrac{2}{\\sqrt{2\\pi}}\\int_{0}^{\\infty}x^{p-1}\\exp\\left(-y\\right)\\mathrm{D}y,\n\\]\nluego \\(\\sqrt{2y}=x,\\) entonces \\[\\begin{align*}\nE\\left[X^{p}\\right] & =\\dfrac{2}{\\sqrt{2\\pi}}\\int_{0}^{\\infty}\\left(\\sqrt{2y}\\right)^{p-1}\\exp\\left(-y\\right)\\mathrm{D}y\\\\\n& =\\dfrac{2\\left(\\sqrt{2}\\right)^{p-1}}{\\sqrt{2\\pi}}\\int_{0}^{\\infty}y^{\\frac{p-1}{2}}\\exp\\left(-y\\right)\\mathrm{D}y\\\\\n& =\\dfrac{2\\left(\\sqrt{2}\\right)^{p-1}}{\\sqrt{2\\pi}}\\int_{0}^{\\infty}y^{\\frac{p+1}{2}-1}\\exp\\left(-y\\right)\\mathrm{D}y,\n\\end{align*}\\] recordando la función Gamma. \\[\n\\Gamma\\left(z\\right)=\\int_{0}^{\\infty}x^{z-1}e^{-t}\\mathrm{D}t,\n\\]\nentonces\n\\[\nE\\left[X^{p}\\right]=\\begin{cases}\n0 & p\\text{ impar}\\\\\n\\dfrac{2^{\\frac{p+1}{2}}}{\\sqrt{2\\pi}}\\Gamma\\left(\\dfrac{p+1}{2}\\right) & p\\text{ par}\n\\end{cases},\n\\]\nentonces \\(E\\left[X^{4}\\right]=\\dfrac{4}{\\sqrt{\\pi}}\\Gamma\\left(\\dfrac{5}{2}\\right)\\)\n\\[\\begin{align*}\n\\Gamma\\left(\\dfrac{5}{2}\\right) & =\\int_{0}^{\\infty}x^{\\frac{3}{2}}e^{-x}\\mathrm{D}x.\n\\end{align*}\\]\nConsidere \\[\\begin{align*}\nu=x^{3/2} & \\mathrm{D}v=e^{-x}\\mathrm{D}x\\\\\n\\mathrm{D}u=\\dfrac{3}{2}x^{1/2} & v=-e^{-x},\n\\end{align*}\\]\n\\[\\begin{align*}\n\\Gamma\\left(\\dfrac{5}{2}\\right) & =\\left[-x^{3/2}e^{-x}\\right]+\\dfrac{3}{2}\\int_{0}^{\\infty}x^{1/2}e^{-x}\\mathrm{D}x\\\\\n& =\\dfrac{3}{2}\\dfrac{\\sqrt{\\pi}}{2}.\n\\end{align*}\\]\nEntonces \\[\nE\\left[X^{4}\\right]=3\n\\]\nNotemos que si \\(\\Delta W\\sim N\\left(0,\\sigma^{2}\\right)\\), entonces \\[\nZ=\\dfrac{\\Delta W}{\\sigma}\\sim N\\left(0,1\\right)\n\\]\nEn general, \\[\nE\\left[\\left(\\Delta W\\right)^{p}\\right]=\\dfrac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}w^{p}\\exp\\left(-\\left(\\dfrac{w}{\\sigma}\\right)^{2}\\right)\\mathrm{D}w,\n\\]\nconsidere \\(\\sigma u=w\\), entonces \\[\\begin{align*}\nE\\left[\\left(\\Delta W\\right)^{p}\\right] & =\\sigma^{p}\\left[\\dfrac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}u^{p}\\exp\\left(-u^{2}\\right)\\mathrm{D}w\\right],\\\\\n& =\\sigma^{p}E\\left[Z^{p}\\right],\n\\end{align*}\\]\nentonces para \\(p=4,\\sigma^{2}=\\Delta t\\). \\[\nE\\left[\\left(\\Delta W\\right)^{4}\\right]=\\left(\\Delta t\\right)^{2}E\\left[Z^{4}\\right]=3\\left(\\Delta t\\right)^{2}\n\\]\n\nExercise 6.5 Suponga que \\(X\\sim N\\left(0,1\\right)\\). Muestre que para \\(a,b\\in\\mathbb{R}\\), \\[\nE\\left[\\exp\\left(a+bX\\right)\\right]=\\exp\\left(a+\\dfrac{1}{2}b^{2}\\right).\n\\] Deduzca que \\[\nE\\left[\\exp\\left(t+\\dfrac{1}{4}W_{t}\\right)\\right]=\\exp\\left(\\dfrac{33}{32}t\\right)\n\\]\n\nConsidere \\[\nE\\left[\\exp\\left(a+bX\\right)\\right]=e^{a}E\\left[\\exp\\left(bX\\right)\\right],\n\\]\nnotemos que \\(bX\\sim N\\left(0,b^{2}\\right),\\) por lo tanto, la función generadora de momentos nos dice que \\[\nE\\left[\\exp\\left(bX\\right)\\right]=M_{bX}\\left(1\\right)=\\exp\\left(\\dfrac{b^{2}}{2}\\right),\n\\]\npor lo tanto \\[\nE\\left[\\exp\\left(a+bX\\right)\\right]=e^{a}\\exp\\left(\\dfrac{b^{2}}{2}\\right)=\\exp\\left(a+\\dfrac{1}{2}b^{2}\\right),\n\\]\nahora, considere \\[\nE\\left[\\exp\\left(t+\\dfrac{1}{4}W_{t}\\right)\\right]=E\\left[\\exp\\left(t+\\dfrac{1}{4}\\left(W_{t}-W_{0}\\right)\\right)\\right],\n\\]\nNotemos que \\(W_{t}-W_{0}\\sim N\\left(0,t\\right),\\) por lo tanto, usando la fórmula anterior\n\\[\\begin{align*}\nE\\left[\\exp\\left(t+\\dfrac{1}{4}\\left(W_{t}-W_{0}\\right)\\right)\\right] & =\\exp\\left(t+\\dfrac{1}{4}\\left(\\sqrt{t}X\\right)\\right),X\\sim N\\left(0,1\\right)\\\\\n& =\\exp\\left(t+\\left(\\dfrac{1}{4}\\sqrt{t}\\right)X\\right)=\\exp\\left(t+\\dfrac{t}{32}\\right)\\\\\n& =\\exp\\left(\\dfrac{33}{32}t\\right)\n\\end{align*}\\]"
  },
  {
    "objectID": "Tarea6.html",
    "href": "Tarea6.html",
    "title": "7  Tarea 6",
    "section": "",
    "text": "Exercise 7.1 Hacer un script para simular 10000 trayectorias del proceso \\(u(t,W_t)\\) definido en el Ejercicio. Grafique en una figura, 10 trayectorias y la media de las 10000 trayectorias del proceso \\(u(t, W_t)\\).\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef strong_brownian(t, n):\n    dt = t / n\n    dw = np.zeros(n)\n    w = np.zeros(n)\n    for i in np.arange(1, n):\n        dw[i] = np.sqrt(dt)*np.random.standard_normal()\n        w[i] = w[i - 1] + dw[i]\n    time = np.linspace(0, t, n)\n    return time, w\n\n\n\ndef b_function(t, a, w):\n    y = np.exp(t - a * w)\n    return y\n\n\nn_samples = 10000\nn_points = 64\nt_initial = 0\nt_final = 1\n\nmean = np.zeros(n_points)\nfor i in range(n_samples):\n    time, b_w = strong_brownian(t_final, n_points)\n    y = b_function(time, 0.25, b_w)\n    if i &lt; 10:\n        plt.plot(time, b_w, 'g-', alpha=0.5)\n    mean += y\n\nmean = (n_samples)**(-1) * mean\ntime = np.linspace(0, t_final, n_points)\n\ny = [np.exp(33 / 32 * t) for t in time]\nplt.plot(time, mean, 'r-')\nplt.plot(time, y, 'b-', alpha=0.3)\nplt.show()\n\n\n\n\n\nExercise 7.2 Siguiendo las ideas del refinamiento del camino browniano. \\(t_{i + 1/2} = t_i + \\dfrac{1}{2}\\delta t\\). Hacer un código de Python para el refinamiento del Browniano para \\(\\alpha \\in (0, 1)\\) para el refinamiento $t_{i+ } = t_i + t $\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef strong_brownian(t, n):\n    dt = t / n\n    dw = np.zeros(n)\n    w = np.zeros(n)\n    for i in np.arange(1, n):\n        dw[i] = np.sqrt(dt)*np.random.standard_normal()\n        w[i] = w[i - 1] + dw[i]\n    time = np.linspace(0, t, n)\n    return time, w\n\nt_final = 1\nn_points = 65\ndelta_t = 1/(n_points - 1)\nalpha = 0.7\n\nprng = np.random.RandomState(219)\n\ntime, w = strong_brownian(1, n_points)  # w_i\n\ny = np.sqrt(delta_t * (alpha - alpha ** 2)) * prng.standard_normal(n_points - 1)\n\nw_ = np.roll(w, -1)  # w_i+1\n\nw_alpha = alpha * w_ + (1 - alpha) * w\nw_alpha = np.delete(w_alpha, -1)\nw_alpha += y\nw_ref = np.zeros(2* n_points -1)\n\nw_ref[0::2] = w\nw_ref[1::2] = w_alpha\n\ntime_ref = np.zeros(2 * n_points - 1)\n\nfor i in range(2 * n_points - 1):\n    if i % 2 == 0:\n        time_ref[i] = time[int(i / 2)]\n    else:\n        time_ref[i] = time[int(i / 2)] + alpha * delta_t\n\nplt.plot(time_ref, w_ref, 'g-')\nplt.plot(time, w, 'ro')\nplt.show()"
  },
  {
    "objectID": "Tarea7.html",
    "href": "Tarea7.html",
    "title": "8  Tarea 7",
    "section": "",
    "text": "Sea \\(W(t)\\) un Movimiento Browniano y \\(Z_{i}\\) una colección de variables aleatorias i.i.d, con distribución \\(N\\left(0,\\frac{\\delta t}{4}\\right)\\).\nPruebe que la suma \\[\n\\sum_{i=0}^{L}Z_{i}\\left(W(t_{i+1})-W(t_{i})\\right),\n\\] tiene valor esperado igual a cero y una varianza de \\(O(\\delta t)\\).\n\nSin perdida de generalidad supongamos que \\(Z_{i}\\) y \\(W(t_{i+1})-W(t_{i})\\) son variables aleatorias independientes para cada \\(i=1,\\dots L\\). Entonces \\[\\begin{eqnarray*}\n    \\mathbb{E}\\left[\\sum_{i=0}^{L}Z_{i}\\left(W(t_{i+1})-W(t_{i})\\right)\\right] & = &  \\sum_{i=0}^{L}\\mathbb{E}\\left[Z_{i}\\left(W(t_{i+1})-W(t_{i})\\right)\\right]\\\\\n    & = &  \\sum_{i=0}^{L}\\mathbb{E}(Z_{i})\\mathbb{E}\\left(W(t_{i+1})-W(t_{i})\\right)\\\\\n    & = & 0\n\\end{eqnarray*}\\] así, \\[\\begin{eqnarray*}\n    Var\\left[\\sum_{i=0}^{L}Z_{i}\\left(W(t_{i+1})-W(t_{i})\\right)\\right] & = &   \\mathbb{E}\\left[\\left(\\sum_{i=0}^{L} Z_{i}(W(t_{i+1})-W(t_{i}))\\right)^{2}\\right]+\\left(\\mathbb{E}\\left[\\sum_{i=0}^{L} Z_{i}(W(t_{i+1})-W(t_{i}))\\right]\\right)^{2}\\\\\n    & = &  \\mathbb{E}\\left[\\left(\\sum_{i=0}^{L} Z_{i}(W(t_{i+1})-W(t_{i}))\\right)^{2}\\right]\n\\end{eqnarray*}\\] por el Teorema multinomial, resulta \\[\\begin{eqnarray*}\n\\mathbb{E}\\left[\\left(\\sum_{i=0}^{L} Z_{i}(W(t_{i+1})-W(t_{i}))\\right)^{2}\\right] & = &  \\mathbb{E}\\left[\\sum_{i=0}^{L} \\left[Z_{i}(W(t_{i+1})-W(t_{i}))\\right]^{2}+2\\sum_{i\\neq j}^{L} Z_{i}(W(t_{i+1})-W(t_{i}))Z_{j}(W(t_{j+1})-W(t_{j}))\\right]\\\\\n& = &  \\sum_{i=0}^{L} \\mathbb{E}\\left[Z_{i}(W(t_{i+1})-W(t_{i}))\\right]^{2}+2\\sum_{i\\neq j}^{L} \\mathbb{E}\\left[Z_{i}(W(t_{i+1})-W(t_{i}))Z_{j}(W(t_{j+1})-W(t_{j}))\\right]\\\\\n& = &  \\sum_{i=0}^{L} \\mathbb{E}\\left[Z_{i}(W(t_{i+1})-W(t_{i}))\\right]^{2}+2\\sum_{i\\neq j}^{L} \\mathbb{E}\\left[Z_{i}(W(t_{i+1})-W(t_{i}))Z_{j}(W(t_{j+1})-W(t_{j}))\\right]\n\\end{eqnarray*}\\] observemos que, si \\(i&lt;j\\) \\[\\begin{eqnarray*}\n    \\mathbb{E}\\left[Z_{i}(W(t_{i+1})-W(t_{i}))Z_{j}(W(t_{j+1})-W(t_{j}))\\right] & = &  \\mathbb{E}\\{ \\mathbb{E}\\left[Z_{i}(W(t_{i+1})-W(t_{i}))Z_{j}(W(t_{j+1})-W(t_{j}))|\\mathcal{F}_{j}\\right]\n    \\}\\\\\n    & = & \\mathbb{E}[Z_{i}(W(t_{i+1})-W(t_{i}))Z_{j} ]\\mathbb{E}\\left[(W(t_{j+1})-W(t_{j}))|\\mathcal{F}_{j}\\right]\\\\\n    & = & 0\n\\end{eqnarray*}\\] y además \\[\\begin{eqnarray*}\n    \\mathbb{E}\\left[Z_{i}(W(t_{i+1})-W(t_{i}))\\right]^{2} & = &  \\mathbb{E}\\{ \\mathbb{E}\\left[Z_{i}^{2}(W(t_{i+1})-W(t_{i}))^{2}|\\mathcal{F}_{j}\\right]\n    \\}\\\\\n    & = & \\mathbb{E}\\{Z_{i}^{2}\\mathbb{E}\\left[(W(t_{i+1})-W(t_{i}))^{2}|\\mathcal{F}_{j}\\right]\\}\\\\\n    & = & \\mathbb{E}[Z_{i}^{2}](t_{i+1}-t_{i})\\\\\n    & = & \\dfrac{\\delta t}{4}(t_{i+1}-t_{i})\n\\end{eqnarray*}\\] sustituyendo resulta \\[\\begin{eqnarray*}\nVar\\left[\\sum_{i=0}^{L}Z_{i}\\left(W(t_{i+1})-W(t_{i})\\right)\\right] & = &  \\sum_{i=0}^{L}\\dfrac{\\delta t}{4}(t_{i+1}-t_{i})\\\\\n& = &  \\dfrac{\\delta t}{4}(t_{L+1}-t_{0}).\n\\end{eqnarray*}\\] Para un \\(L\\) suficientemente grande, se tiene que, \\((t_{L+1}-t_{0})\\leq\\dfrac{\\varepsilon}{4}\\), así \\[\nVar\\left[\\sum_{i=0}^{L}Z_{i}\\left(W(t_{i+1})-W(t_{i})\\right)\\right]\\leq \\varepsilon\\delta t.\n\\] Así, la varianza es de orden \\(\\delta t\\).\n\n2.- La regla del punto medio de la integral de Riemann de una función \\(h\\in C^{2}([a,b])\\) sobre una partición de \\(L\\) puntos del intervalo \\([a,b]\\) está dada por, \\[\n\\int_{a}^{b}h(t)dt=\\lim_{\\delta t\\to 0,\\,L\\to\\infty}\\sum_{i=0}^{L}h\\left(\\dfrac{t_{i}+t_{i+1}}{2}\\right)\\delta t.\n\\] Use la relación \\[\nW\\left(\\frac{t_{i}+t_{i+1}}{2}\\right)=\\frac{1}{2}(W(t_{i})+W(t_{i+1}))+ \\underbrace{Z_{i}}_{i.i.d.\\sim N(0,\\delta t/4)},\n\\] y el ejercicio anterior para demostrar que la regla del punto medio de la integral de Riemann implica que \\[\n\\int_{0}^{T}W(t)dW(t)=\\frac{1}{2}W(T)^{2}.\n\\]\n\nSea \\(\\Delta_{L}=\\{0=t_{0},t_{1},\\dots,t_{L-1},t_{L}=T\\}\\) una partición del intervalo \\([0,T]\\). De la regla del punto medio para \\(h(t)=W(t)\\), resulta \\[\\begin{eqnarray*}\n    \\int_{0}^{T}W(t)dW(t) & = & \\lim_{\\delta t\\to 0,\\,L\\to\\infty}\\sum_{i=0}^{L}W\\left(\\dfrac{t_{i}+t_{i+1}}{2}\\right)(W(t_{i+1})-W(t_{i}))\\\\\n    & = & \\lim_{\\delta t\\to 0,\\,L\\to\\infty}\\sum_{i=0}^{L}\\left[\\frac{1}{2}(W(t_{i})+W(t_{i+1}))+ Z_{i}\\right](W(t_{i+1})-W(t_{i}))\\\\\n    & = & \\lim_{\\delta t\\to 0,\\,L\\to\\infty}\\sum_{i=0}^{L}\\frac{1}{2}\\left(W(t_{i+1})^{2}-W(t_{i})^{2}\\right)+ \\lim_{\\delta t\\to 0,\\,L\\to\\infty}\\sum_{i=0}^{L} Z_{i}(W(t_{i+1})-W(t_{i}))\\\\\n    & = & \\lim_{\\delta t\\to 0,\\,L\\to\\infty}\\frac{1}{2}\\left(W(T)^{2}-W(0)^{2}\\right)+ \\lim_{\\delta t\\to 0,\\,L\\to\\infty}\\sum_{i=0}^{L} Z_{i}(W(t_{i+1})-W(t_{i}))\\\\\n    & = & \\frac{1}{2}W(T)^{2}+ \\lim_{\\delta t\\to 0,\\,L\\to\\infty}\\sum_{i=0}^{L} Z_{i}(W(t_{i+1})-W(t_{i}))\n\\end{eqnarray*}\\] Solo falta ver que \\[\n\\sum_{i=0}^{L} Z_{i}(W(t_{i+1})-W(t_{i}))\\rightarrow 0\\text{ en } L^{2}\n\\] es decir, \\[\n\\lim_{\\|\\Delta_{L}\\|\\to0}E\\left[\\left(\\sum_{i=0}^{L} Z_{i}(W(t_{i+1})-W(t_{i}))\\right)^{2}\\right]=0\n\\] Del ejercicio anterior se tiene que \\[\nE\\left[\\left(\\sum_{i=0}^{L} Z_{i}(W(t_{i+1})-W(t_{i}))\\right)^{2}\\right] = O(\\delta t)\\leq \\varepsilon\\|\\Delta_{L}\\|,\n\\] así, tomando el limite cuando \\(\\|\\Delta_{L}\\|\\to\\) se tiene que, \\[\n\\sum_{i=0}^{L} Z_{i}(W(t_{i+1})-W(t_{i}))\\rightarrow 0\\text{ en } L^{2}\n\\]\n\nUsando la aproximación de la suma de Riemann \\[\n\\int_{0}^{T}h(t)dW(t)\\sim\\sum_{i=0}^{L}h(t_{i})(W(t_{i+1})-W(t_{i})),\n\\] argumente que, \\[\n\\mathbb{E}\\left[\\left(\\int_{0}^{T}tdW(t)\\right)^{2}\\right]=\\frac{T^{3}}{3}.\n\\]\nPor tanto, enuncie la isometría de It^o y deduzca que esta isometría es válida para el caso \\(h(t)=t\\).\n\nSea \\(\\{0=t_{0},t_{1},\\dots,t_{L-1},t_{L}=T\\}\\) una partición del intervalo \\([0,T]\\). De la aproximación de la suma de Riemann, resulta \\[\\begin{eqnarray*}\n\\int_{0}^{T}tdW(t) & \\sim & \\sum_{i=0}^{L}t_{i}(W(t_{i+1})-W(t_{i}))\\\\\n\\Longrightarrow \\left(\\int_{0}^{T}tdW(t)\\right)^{2} & \\sim & \\left(\\sum_{i=0}^{L}t_{i}(W(t_{i+1})-W(t_{i}))\\right)^{2},\n\\end{eqnarray*}\\] por el Teorema Multinomial, resulta \\[\\begin{equation*}\n    \\left(\\sum_{i=0}^{L}t_{i}(W(t_{i+1})-W(t_{i}))\\right)^{2} = \\sum_{i=0}^{L}t_{i}^{2}(W(t_{i+1})-W(t_{i}))^{2}+2\\sum_{i\\neq j}t_{i} t_{j}(W(t_{i+1})-W(t_{i}))(W(t_{j+1})-W(t_{j}))\n\\end{equation*}\\] entonces sustituyendo esto, resulta \\[\\begin{eqnarray*}\n   \\mathbb{E}\n\\left[\\left(\\int_{0}^{T}tdW(t)\\right)^{2}\\right] & \\sim &  \\mathbb{E}\n\\left[\\sum_{i=0}^{L}t_{i}^{2}(W(t_{i+1})-W(t_{i}))^{2}+2\\sum_{i\\neq j}t_{i} t_{j}(W(t_{i+1})-W(t_{i}))(W(t_{j+1})-W(t_{j}))\\right]\\\\\n& = &  \n\\sum_{i=0}^{L}t_{i}^{2}\\mathbb{E}(W(t_{i+1})-W(t_{i}))^{2}+2\\sum_{i\\neq j}t_{i} t_{j}\\mathbb{E}\\left[(W(t_{i+1})-W(t_{i}))(W(t_{j+1})-W(t_{j}))\\right]\\\\\n& = &  \n\\sum_{i=0}^{L}t_{i}^{2}(t_{i+1}-t_{i}),\n\\end{eqnarray*}\\] observemos que, \\[\n\\lim_{L\\to 0}\\sum_{i=0}^{L}t_{i}^{2}(t_{i+1}-t_{i})=\\int_{0}^{T}t^{2}dt=\\frac{T}{3}\n\\] entonces \\[\n\\mathbb{E}\n\\left[\\left(\\int_{0}^{T}tdW(t)\\right)^{2}\\right]=\\frac{T}{3}.\n\\] Además, de la isometria de Itô, \\[\\begin{eqnarray*}\n\\mathbb{E}\n\\left[\\left(\\int_{0}^{T}tdW(t)\\right)^{2}\\right] & = & \\mathbb{E}\n\\left[\\left(\\int_{0}^{T}tdW(t)\\right)\\left(\\int_{0}^{T}tdW(t)\\right)\\right]\\\\\n& = &\n\\int_{0}^{T}\\mathbb{E}(t^{2})dt\\\\\n& = &\n\\int_{0}^{T}t^{2}dt\\\\\n& = & \\frac{T}{3}\n\\end{eqnarray*}\\]\n\nEscriba una función de Python para calcular la integral de It^o del movimiento Browniano \\(W(t)\\) sobre \\([0,T]\\). La función tendría la siguiente firma."
  },
  {
    "objectID": "Tarea8.html",
    "href": "Tarea8.html",
    "title": "9  Tarea 8",
    "section": "",
    "text": "Use la aproximación de la suma de Riemann la ecuación 6.1. Muestra la propiedad de linealidad de la integral estocástica. Es decir, \\[\n\\int_{0}^{T}\\left(\\alpha f(t)+\\beta g(t)\\right) dW_{t}=\\alpha\\int_{0}^{T}f(t)dW_{t}+\\beta\\int_{0}^{T}g(t)dW_{t}\n\\]\n\nSea \\(\\{0=t_{0},t_{1},\\dots,t_{L-1},t_{L}=T\\}\\) una particion del intervalo \\([0,T]\\), de la aproximación de la suma de Riemann, resulta \\[\\begin{eqnarray*}\n    \\int_{0}^{T}\\left(\\alpha f(t)+\\beta g(t)\\right) dW_{t} & \\sim & \\sum_{i=0}^{L}(\\alpha f(t_{i})+\\beta g(t_{i}))(W(t_{i+1})-W(t_{i}))\\\\\n    & = & \\sum_{i=0}^{L}\\alpha f(t_{i})(W(t_{i+1})-W(t_{i}))+ \\sum_{i=0}^{L}\\beta g(t_{i})(W(t_{i+1})-W(t_{i}))\\\\\n    & = & \\alpha\\sum_{i=0}^{L} f(t_{i})(W(t_{i+1})-W(t_{i}))+\\beta\\sum_{i=0}^{L} g(t_{i})(W(t_{i+1})-W(t_{i}))\n\\end{eqnarray*}\\]\ntomando el limite cuando \\(L\\to\\infty\\), resulta \\[\n    \\alpha\\lim_{L\\to\\infty}\\sum_{i=0}^{L} f(t_{i})(W(t_{i+1})-W(t_{i}))=\\alpha \\int_{0}^{T}f(t)dW_{t}\n\\]\ny \\[\n    \\beta\\lim_{L\\to\\infty}\\sum_{i=0}^{L} g(t_{i})(W(t_{i+1})-W(t_{i}))=\\beta\\int_{0}^{T}g(t)dW_{t}\n\\]\nasí, \\[\n\\int_{0}^{T}\\left(\\alpha f(t)+\\beta g(t)\\right) dW_{t}= \\alpha \\int_{0}^{T}f(t)dW_{t}+\\beta\\int_{0}^{T}g(t)dW_{t}\n\\]\n\nEscriba con detalle la demostración del siguiente Teorema, también incluya la demostración del Lema 5.18 del Mao.\\ Teorema: Sea \\(f\\in\\mathcal{M}^{2}([0,T];\\mathbb{R})\\), sea \\(\\rho,\\tau\\) dos tiempos de paro tales que \\(0\\leq\\rho\\leq\\tau\\leq T\\). Entonces \\[\\begin{eqnarray}\n    \\mathbb{E}\\left(\\int_{\\rho}^{\\tau}f(s)dW_{s}\\mid\\mathcal{F}_{\\rho}\\right) & = & 0,\\\\\n    \\mathbb{E}\\left(\\left|\\int_{\\rho}^{\\tau}f(s)dW_{s}\\right|^{2}\\mid\\mathcal{F}_{\\rho}\\right) & = & \\mathbb{E}\\left(\\int_{\\rho}^{\\tau}\\left|f(s)\\right|^{2}ds\\mid\\mathcal{F}_{\\rho}\\right).\n\\end{eqnarray}\\]\n\nPor el Teorema 5.14 y el teorema de paro de la martingala de Doob, \\[\\begin{equation}\n    E(I(\\tau)|\\mathcal{F}_{\\rho})=I(\\rho)\n\\end{equation}\\] y \\[\\begin{equation}\n    E(I^{2}(\\tau)-\\langle I,I\\rangle_{\\tau}|\\mathcal{F}_{\\rho})=I^{2}(\\rho)-\\langle I,I\\rangle_{\\rho},\n\\end{equation}\\] donde \\(\\{\\langle I,I\\rangle_{t}\\}\\) es definido por 5.18. Aplicando el Lema 5.18 se ve entonces de 5.22 que \\[\n\\mathbb{E}\\left(\\int_{\\rho}^{\\tau}f(s)dB_{s}|\\mathcal{F}_{\\rho}\\right)=\\mathbb{E}(I(\\tau)-I(\\rho)|\\mathcal{F}_{\\rho})=0\n\\]\nque es (5.20). Además, por (5.22) y (5.23),\n\\[\n\\mathbb{E}(|I(\\tau)-I(\\rho)|^{2}|\\mathcal{F}_{\\rho})=\\mathbb{E}(I^{2}(\\tau)|\\mathcal{F}_{\\rho})-2I(\\rho)\\mathbb{E}(I(\\tau)|\\mathcal{F}_{\\rho})+I^{2}(\\rho)\n\\]\n\\[\n=\\mathbb{E}(I^{2}(\\tau)|\\mathcal{F}_{\\rho})-I^{2}(\\rho)=\\mathbb{E}(\\langle I,I\\rangle_{\\tau}-\\langle I, I\\rangle_{\\rho}|\\mathcal{F}_{\\rho})=\\mathbb{E}\\left(\\int_{\\rho}^{\\tau}|f(s)|^{2}ds|\\mathcal{F}_{\\rho}\\right)\n\\]\n\nUsando la aproximación de la suma de Riemann ecuación 6.1, la isometría de Itô y la identidad \\(4ab=(a+b)^{2}-(a-b)^{2}\\) pruebe que \\[\n\\mathbb{E}\\left[\\left(\\int_{0}^{T}g(t)dW_{t}\\right)\\left(\\int_{0}^{T}f(t)dW_{t}\\right)\\right]=\\int_{0}^{T}\\mathbb{E}[f(t)g(t)]dt.\n\\]\n\nTomemos \\(a=\\int_{0}^{T}g(t)dW_{t}\\) y \\(b=\\int_{0}^{T}f(t)dW_{t}\\), entonces usando la identidad \\(4ab=(a+b)^{2}-(a-b)^{2}\\) \\[\\begin{eqnarray*}\n    4\\left(\\int_{0}^{T}g(t)dW_{t}\\right)\\left(\\int_{0}^{T}f(t)dW_{t}\\right) & = & \\left(\\int_{0}^{T}g(t)dW_{t}+\\int_{0}^{T}f(t)dW_{t}\\right)^{2}-\\left(\\int_{0}^{T}g(t)dW_{t}-\\int_{0}^{T}f(t)dW_{t}\\right)^{2}\\\\\n    & = & \\left(\\int_{0}^{T}(g(t)+f(t))dW_{t}\\right)^{2}-\\left(\\int_{0}^{T}(g(t)-f(t))dW_{t}\\right)^{2},\n\\end{eqnarray*}\\] así, \\[\\begin{eqnarray*}\n    4\\mathbb{E}\\left[\\left(\\int_{0}^{T}g(t)dW_{t}\\right)\\left(\\int_{0}^{T}f(t)dW_{t}\\right)\\right] & = & \\mathbb{E}\\left(\\int_{0}^{T}(g(t)+f(t))dW_{t}\\right)^{2}-\\mathbb{E}\\left(\\int_{0}^{T}(g(t)-f(t))dW_{t}\\right)^{2}\\\\\n   & = &\\left(\\int_{0}^{T} \\mathbb{E}(g(t)+f(t))^{2}dt\\right)-\\left(\\int_{0}^{T}\\mathbb{E}(g(t)-f(t))^{2}dt\\right)\\\\\n    & = & \\left(\\int_{0}^{T}\\mathbb{E}[(g(t)+f(t))^{2}-(g(t)-f(t))^{2}]dt\\right)\\\\\n    & = & 4\\left(\\int_{0}^{T}\\mathbb{E}[g(t)f(t)]dt\\right)\n\\end{eqnarray*}\\]\n\nUsando la suma de Riemann ecuación 6.1 y deduzca que, \\[\n\\int_{0}^{T}W(t)^{2}dW(t)=\\dfrac{1}{3}W(T)^{3}-\\int_{0}^{T}W(t)dt.\n\\]\n\nObservemos que, \\[\n3W(t_{i})^{2}(W(t_{i+1})-W((t_{i})))=W(t_{i+1})^{3}-\\left(W(t_{i+1})-W(t_{i})\\right)^{3}-3\\left(W(t_{i+1})-W(t_{i})\\right)^{2}W(t_{i})-W(t_{i-1})^{3},\n\\]\naplicando la ecuación 6.1 \\[\\begin{align*}\n\\int_{0}^{T}W(t)^{2}dW(t) & \\sim \\sum_{i=0}^{L}W(t_{i})^{2} (W(t_{i+1})-W((t_{i})))\\\\\n& = \\frac{1}{3} \\sum_{i=0}^{L}\\left[W(t_{i+1})^{3}-W(t_{i-1})^{3}\\right]-\\frac{1}{3}\\sum_{i=0}^{L}\\left(W(t_{i+1})-W(t_{i})\\right)^{3}-\\sum_{i=0}^{L}\\left(W(t_{i+1})-W(t_{i})\\right)^{2}W(t_{i})\\\\\n& =\\frac{1}{3}( W(T)^{3}-W(t_{0})^{3})- \\frac{1}{3}\\sum_{i=0}^{L}\\left(W(t_{i+1})-W(t_{i})\\right)^{3}-\\sum_{i=0}^{L}\\left(W(t_{i+1})-W(t_{i})\\right)^{2}W(t_{i})\\\\\n  & =\\frac{1}{3}W(T)^{3}- \\frac{1}{3}\\sum_{i=0}^{L}\\left(W(t_{i+1})-W(t_{i})\\right)^{3}-\\sum_{i=0}^{L}\\left(W(t_{i+1})-W(t_{i})\\right)^{2}W(t_{i})\n\\end{align*}\\] veamos que \\(\\frac{1}{3}\\sum_{i=0}^{L}\\left(W(t_{i+1})-W(t_{i})\\right)^{3}\\to 0\\) en \\(L^{2}\\).\\ Ahora, calcularemos la media de la variación cuadrática. Del Teorema Multinomial \\[\n\\frac{1}{9}\\mathbb{E}\\left[\\left(\\sum_{i=0}^{L}\\left(W(t_{i+1})-W(t_{i})\\right)^{3}\\right)^{2}\\right]=\\frac{1}{9}\\sum_{i=0}^{L}\\mathbb{E}\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{6}\\right]+\\frac{2}{9}\\sum_{i=0}^{L}\\mathbb{E}\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{3}\\left(W(t_{j+1})-W(t_{j})\\right)^{3}\\right],\n\\]\nademás, de la tarea 5, \\[\\begin{eqnarray*}\n    \\mathbb{E}\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{3}\\left(W(t_{j+1})-W(t_{j})\\right)^{3}\\right] & = & \\mathbb{E}\\{\\mathbb{E}\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{3}\\left(W(t_{j+1})-W(t_{j})\\right)^{3}\\right]|\\mathcal{F}_{j}\\}\\\\\n    & = & \\mathbb{E}\\{\\left(W(t_{i+1})-W(t_{i})\\right)^{3}\\mathbb{E}\\left[\\left(W(t_{j+1})-W(t_{j})\\right)^{3}\\right|\\mathcal{F}_{j}]\\}\\\\\n    & = & \\mathbb{E}[\\left(W(t_{i+1})-W(t_{i})\\right)^{3}]\\mathbb{E}\\left[\\left(W(t_{j+1})-W(t_{j})\\right)^{3}\\right]\\\\\n    & = & 0. \\text{ de la tarea 5}\n\\end{eqnarray*}\\] así, \\[\n\\frac{2}{9}\\sum_{i=0}^{L}\\mathbb{E}\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{3}\\left(W(t_{j+1})-W(t_{j})\\right)^{3}\\right]=0\n\\]\ny también se tiene que \\(\\mathbb{E}\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{6}\\right] = 15\\left(t_{i+1}-t_{i}\\right)^{3}\\), así \\[\\begin{align*}\n\\frac{1}{9}\\sum_{i=0}^{L}E\\left[\\left(W(t_{i+1})-W(t_{i})\\right)^{6}\\right] & =\\frac{5}{3}\\sum_{i=0}^{L}\\left(t_{i+1}-t_{i}\\right)^{3}\\\\\n& \\leq\\frac{5}{3}\\|\\Delta_{L}\\|^{2}\\sum_{i=0}^{L}\\left(t_{i+1}-t_{i}\\right)\\\\\n& \\leq\\frac{5}{3}\\|\\Delta_{L}\\|^{2}L\\to0,\\|\\Delta_{L}\\|\\to0\n\\end{align*}\\] Ahora veamos que \\[\n\\sum_{i=0}^{L}\\left(W(t_{i+1})-W(t_{i})\\right)^{2}W(t_{i})\\to \\sum_{i=0}^{L}W(t_{i})\\left(t_{i+1}-t_{i}\\right)\\text{ en }L^{2}\n\\]\nse tiene que, \\[\\begin{eqnarray*}\n  \\mathbb{E}\\left[\\left(\\sum_{i=0}^{L}\\left(W(t_{i+1})-W(t_{i})\\right)^{2}W(t_{i})- \\sum_{i=0}^{L}W(t_{i})\\left(t_{i+1}-t_{i}\\right) \\right)^{2}\\right] & = & \\mathbb{E}\\left[\\left(\\sum_{i=0}^{L}W(t_{i})[\\left(W(t_{i+1})-W(t_{i})\\right)^{2}-\\left(t_{i+1}-t_{i}\\right)] \\right)^{2}\\right]\n\\end{eqnarray*}\\] \\[\n\\mathbb{E}\\left[\\sum_{i=0}^{L}W(t_{i})^{2}[\\left(W(t_{i+1})-W(t_{i})\\right)^{2}-\\left(t_{i+1}-t_{i}\\right)]^{2} +\\sum_{i=0}^{L}W(t_{i})W(t_{j})[\\left(W(t_{i+1})-W(t_{i})\\right)^{2}-\\left(t_{i+1}-t_{i}\\right)][\\left(W(t_{j+1})-W(t_{j})\\right)^{2}-\\left(t_{j+1}-t_{j}\\right)] \\right]\n\\]\ncalculemos \\[\\begin{eqnarray*}\n\\mathbb{E}\\{\\mathbb{E}[W(t_{i})W(t_{j})(\\left(W(t_{i+1})-W(t_{i})\\right)^{2}-\\left(t_{i+1}-t_{i}\\right))(\\left(W(t_{j+1})-W(t_{j})\\right)^{2}-\\left(t_{j+1}-t_{j}\\right))]|\\mathcal{F}_{j}\\} & = &   \n\\end{eqnarray*}\\] \\[\n\\mathbb{E}\\{W(t_{i})W(t_{j})(\\left(W(t_{i+1})-W(t_{i})\\right)^{2}-\\left(t_{i+1}-t_{i}\\right))\\mathbb{E}[(\\left(W(t_{j+1})-W(t_{j})\\right)^{2}-\\left(t_{j+1}-t_{j}\\right))]|\\mathcal{F}_{j}\\} =0\n\\]\n\nVerifique que la isometría de Itô ecuación 6.4, \\[\n\\mathbb{E}\\left[\\left(\\int_{0}^{T}h(t)dW(t)\\right)^{2}\\right]=\\mathbb{E}\\left[\\int_{0}^{T}h(t)^{2}dt\\right],\n\\]\nse tiene cuando \\(h(t):= 1\\).\n\ndel ejercicio 3, resulta \\[\\begin{eqnarray*}\n    \\mathbb{E}\\left[\\left(\\int_{0}^{T}h(t)dW(t)\\right)^{2}\\right]=\\mathbb{E}\\left[\\left(\\int_{0}^{T}1dW(t)\\right)^{2}\\right] & = & \\mathbb{E}\\left[\\left(\\int_{0}^{T}1dW_{t}\\right)\\left(\\int_{0}^{T}1dW_{t}\\right)\\right]\\\\\n    & = & \\int_{0}^{T}\\mathbb{E}[1]dt\\\\\n    & = & \\int_{0}^{T}dt\\\\\n    & = & T.\n\\end{eqnarray*}\\] y \\[\\begin{eqnarray*}\n    \\mathbb{E}\\left[\\int_{0}^{T}h(t)^{2}dt\\right] & = & \\mathbb{E}\\left[\\int_{0}^{T}1^{2}dt\\right]\\\\\n    & = & \\mathbb{E}\\left[\\int_{0}^{T}dt\\right]\\\\\n    & = & \\mathbb{E}\\left[T\\right]\\\\\n    & = & T\n\\end{eqnarray*}\\]\nAsí, \\[\n\\mathbb{E}\\left[\\left(\\int_{0}^{T}dW(t)\\right)^{2}\\right]= \\mathbb{E}\\left[\\int_{0}^{T}dt\\right]\n\\]"
  },
  {
    "objectID": "Tarea9.html",
    "href": "Tarea9.html",
    "title": "10  Tarea 9",
    "section": "",
    "text": "Sea \\(\\tau\\) un tiempo de paro. Prueba que \\(W\\left(t+\\tau\\right)-W\\left(\\tau\\right)\\) es un movimiento browniano.\n\nDefinamos \\[\nV_{\\tau}\\left(t\\right)=W\\left(t+\\tau\\right)-W\\left(\\tau\\right),\n\\]\nnotemos que \\[\nV_{\\tau}\\left(0\\right)=0,\n\\]\nLuego, considere para \\(s\\leq t\\) \\[\\begin{align*}\nV_{\\tau}\\left(t\\right)-V_{\\tau}\\left(s\\right) & =W\\left(t+\\tau\\right)-W\\left(\\tau\\right)-\\left[W\\left(s+\\tau\\right)-W\\left(\\tau\\right)\\right]\\\\\n& =W\\left(t+\\tau\\right)-W\\left(s+\\tau\\right)\\sim N\\left(0,t-s\\right),\n\\end{align*}\\]\nesto además nos garantiza la independencia de los incrementos del Browniano.\n\nSea \\(W_{1}\\left(t\\right),W_{2}\\left(t\\right)\\) movimientos brownianos independientes con punto inicial \\(\\left(W_{1}\\left(0\\right),W_{2}\\left(0\\right)\\right)\\neq\\left(0,0\\right)\\). Defina \\(X_{t}=\\ln\\left(W_{1}^{2}\\left(t\\right)+W_{2}^{2}\\left(t\\right)\\right).\\)\n\n\n10.0.1 Muestre que \\(X_{t}\\) es una martingala local.\nSupongamos que \\(X_{t}\\) NO es una martingala local.\n\n\n10.0.2 Muestre que \\(E\\left|X_{t}\\right|&lt;\\infty\\) para cada \\(t&gt;0\\).\nConsidere \\[\\begin{align*}\nX_{t} & =\\ln\\left(W_{1}^{2}\\left(t\\right)+W_{2}^{2}\\left(t\\right)\\right),\\\\\n\\exp\\left(X_{t}\\right) & =W_{1}^{2}\\left(t\\right)+W_{2}^{2}\\left(t\\right).\n\\end{align*}\\]\n\\[\\begin{align*}\nE\\left[\\exp\\left(X_{t}\\right)\\right] & =E\\left[W_{1}^{2}\\left(t\\right)\\right]+E\\left[W_{2}^{2}\\left(t\\right)\\right]\\\\\n& =2t,\n\\end{align*}\\]\nComo \\(X_{t}\\geq0,\\forall t\\) \\[\\begin{align*}\nX_{t} & \\le\\exp\\left(X_{t}\\right)\\\\\nE\\left[X_{t}\\right] & \\leq2t&lt;\\infty,\\forall t\n\\end{align*}\\]\n\n\n10.0.3 Muestre que \\(X_{t}\\) no es una martingala.\nSupongamos que existe \\(c\\in\\mathbb{R}\\) tal que \\(E\\left[X_{t}\\right]=c,\\forall t\\). Entonces \\[\\begin{align*}\nE\\left[\\ln\\left(W_{1}^{2}\\left(t\\right)+W_{2}^{2}\\left(t\\right)\\right)\\right] & =c\\\\\n\\int_{0}^{\\infty}\\ln\\left(W_{1}^{2}\\left(t\\right)+W_{2}^{2}\\left(t\\right)\\right)\\mathrm{d}\\mathcal{P} & =c,\n\\end{align*}\\]\nComo la integral es finita. Entonces \\[\nX_{t}\\to0,t\\to\\infty,\\text{ c.s}\n\\]\nLuego, \\[\nW_{1}^{2}\\left(t\\right)+W_{2}^{2}\\left(t\\right)\\to1,t\\to\\infty,\\text{c.s}\n\\]\nSin embargo \\[\nE\\left[W_{1}^{2}\\left(t\\right)+W_{2}^{2}\\left(t\\right)\\right]=2t\\to\\infty,t\\to\\infty,\n\\]\nentonces llegamos a una contradicci'on. Entonces \\(E\\left[X_{t}\\right]\\) no es constante, por lo tanto \\(X_{t}\\) no puede ser martingala.\nConsidere \\[\n\\tau_{n}=\\inf_{t}\\left\\{ X_{t}=n\\right\\}\n\\]\nComo \\(X_{t}\\) es no acotada. Entonces \\[\n\\tau_{n}\\left(\\omega\\right)\\to\\infty,n\\to\\infty,\\forall n.\n\\]\nAhora probaremos que \\(Y_t\\) es una martingala. Ahora, considere \\[\nY_{t}=X_{\\min\\left\\{ t,\\tau_{n}\\right\\} },\n\\]\nes adaptado con respecto a la filtración. Si \\(\\tau_{n}&gt;t\\) lo tenemos por construcción. En caso contrario, para \\(n\\in\\mathbb{N}\\).\n\\[\nY_{t}=n\n\\]\n\\[\n\\left[Y_{t}=n\\right]\\subset\\left[\\tau_{n}&lt;t\\right]\\in\\mathcal{F}_{t},\n\\]\npor ser tiempo de paro. Por lo tanto \\(Y_{t}\\) es adaptado a la filtración, por lo tanto nos queda probar que es una martingala.\nConsidere \\(s&lt;t\\). \\[\\begin{align*}\nE\\left[Y_{t}\\mid\\mathcal{F}_{s}\\right] & =E\\left[X_{\\min\\left\\{ t,\\tau_{n}\\right\\} }\\mid\\mathcal{F}_{s}\\right]\\\\\n& =E\\left[X_{t}1_{[t&lt;\\tau_{n}]}\\left(t\\right)\\mid\\mathcal{F}_{s}\\right]+E\\left[X_{t}1_{\\left[\\tau_{n}\\leq t\\right]}\\left(t\\right)\\mid\\mathcal{F}_{s}\\right]\\\\\n& =E\\left[X_{s}1_{[s&lt;\\tau_{n}]}\\left(t\\right)\\mid\\mathcal{F}_{s}\\right]+E\\left[X_{\\tau_{n}}1_{\\left[\\tau_{n}\\leq t\\right]}\\left(t\\right)\\mid\\mathcal{F}_{s}\\right]\\\\\n& =X_{s}1_{[s&lt;\\tau_{n}]}\\left(t\\right)+X_{\\tau_{n}}1_{\\left[\\tau_{n}\\leq s\\right]}\\left(s\\right)\\\\\n& =Y_{s},\n\\end{align*}\\] teniendo así que para cada \\(n\\) \\(Y_{t}\\) es una martingala."
  }
]